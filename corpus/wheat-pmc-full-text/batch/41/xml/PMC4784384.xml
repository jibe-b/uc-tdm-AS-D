<?xml version="1.0"?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName A++V2.4.dtd?>
  <?SourceDTD.Version 2.4?>
  <?ConverterInfo.XSLTName springer2nlmx2.xsl?>
  <?ConverterInfo.Version 2?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">BMC Genomics</journal-id>
      <journal-id journal-id-type="iso-abbrev">BMC Genomics</journal-id>
      <journal-title-group>
        <journal-title>BMC Genomics</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1471-2164</issn>
      <publisher>
        <publisher-name>BioMed Central</publisher-name>
        <publisher-loc>London</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmcid">4784384</article-id>
      <article-id pub-id-type="publisher-id">2553</article-id>
      <article-id pub-id-type="doi">10.1186/s12864-016-2553-1</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Genome-enabled prediction using probabilistic neural network classifiers</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Gonz&#xE1;lez-Camacho</surname>
            <given-names>Juan Manuel</given-names>
          </name>
          <address>
            <email>jmgc@colpos.mx</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Crossa</surname>
            <given-names>Jos&#xE9;</given-names>
          </name>
          <address>
            <email>j.crossa@cgiar.org</email>
          </address>
          <xref ref-type="aff" rid="Aff2"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>P&#xE9;rez-Rodr&#xED;guez</surname>
            <given-names>Paulino</given-names>
          </name>
          <address>
            <email>perpdgo@gmail.com</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ornella</surname>
            <given-names>Leonardo</given-names>
          </name>
          <address>
            <email>leonardoornella@gmail.com</email>
          </address>
          <xref ref-type="aff" rid="Aff3"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Gianola</surname>
            <given-names>Daniel</given-names>
          </name>
          <address>
            <email>gianola@ansci.wisc.edu</email>
          </address>
          <xref ref-type="aff" rid="Aff4"/>
        </contrib>
        <aff id="Aff1"><label/>Colegio de Postgraduados, Campus Montecillo, Texcoco, M&#xE9;xico 056230 M&#xE9;xico </aff>
        <aff id="Aff2"><label/>Biometrics and Statistics Unit (BSU), International Maize and Wheat Improvement Center (CIMMYT), Apdo Postal 6-641, M&#xE9;xico DF, 06600 24105 M&#xE9;xico </aff>
        <aff id="Aff3"><label/>NIDERA SEMILLAS S.A., Ruta 8 Km. 376, 2600 Venado Tuerto, Argentina </aff>
        <aff id="Aff4"><label/>Department of Animal Sciences, University of Wisconsin, Madison, 53706 USA </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>9</day>
        <month>3</month>
        <year>2016</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>9</day>
        <month>3</month>
        <year>2016</year>
      </pub-date>
      <pub-date pub-type="collection">
        <year>2016</year>
      </pub-date>
      <volume>17</volume>
      <elocation-id>208</elocation-id>
      <history>
        <date date-type="received">
          <day>3</day>
          <month>10</month>
          <year>2015</year>
        </date>
        <date date-type="accepted">
          <day>29</day>
          <month>2</month>
          <year>2016</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; Gonz&#xE1;lez-Camacho et al. 2016</copyright-statement>
        <license license-type="OpenAccess">
          <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
        </license>
      </permissions>
      <abstract id="Abs1">
        <sec>
          <title>Background</title>
          <p>Multi-layer perceptron (MLP) and radial basis function neural networks (RBFNN) have been shown to be effective in genome-enabled prediction. Here, we evaluated and compared the classification performance of an MLP classifier versus that of a probabilistic neural network (PNN), to predict the probability of membership of one individual in a phenotypic class of interest, using genomic and phenotypic data as input variables. We used 16 maize and 17 wheat genomic and phenotypic datasets with different trait-environment combinations (sample sizes ranged from 290 to 300 individuals) with 1.4&#xA0;k and 55&#xA0;k SNP chips. Classifiers were tested using continuous traits that were categorized into three classes (upper, middle and lower) based on the empirical distribution of each trait, constructed on the basis of two percentiles (15&#x2013;85&#xA0;% and 30&#x2013;70&#xA0;%). We focused on the 15 and 30&#xA0;% percentiles for the upper and lower classes for selecting the best individuals, as commonly done in genomic selection. Wheat datasets were also used with two classes. The criteria for assessing the predictive accuracy of the two classifiers were the area under the receiver operating characteristic curve (<italic>AUC</italic>) and the area under the precision-recall curve (<italic>AUCpr</italic>). Parameters of both classifiers were estimated by optimizing the <italic>AUC</italic> for a specific class of interest.</p>
        </sec>
        <sec>
          <title>Results</title>
          <p>The <italic>AUC</italic> and <italic>AUCpr</italic> criteria provided enough evidence to conclude that PNN was more accurate than MLP for assigning maize and wheat lines to the correct upper, middle or lower class for the complex traits analyzed. Results for the wheat datasets with continuous traits split into two and three classes showed that the performance of PNN with three classes was higher than with two classes when classifying individuals into the upper and lower (15 or 30&#xA0;%) categories.</p>
        </sec>
        <sec>
          <title>Conclusions</title>
          <p>The PNN classifier outperformed the MLP classifier in all 33 (maize and wheat) datasets when using <italic>AUC</italic> and <italic>AUCpr</italic> for selecting individuals of a specific class. Use of PNN with Gaussian radial basis functions seems promising in genomic selection for identifying the best individuals. Categorizing continuous traits into three classes generally provided better classification than when using two classes, because classification accuracy improved when classes were balanced.</p>
        </sec>
        <sec>
          <title>Electronic supplementary material</title>
          <p>The online version of this article (doi:10.1186/s12864-016-2553-1) contains supplementary material, which is available to authorized users.</p>
        </sec>
      </abstract>
      <kwd-group xml:lang="en">
        <title>Keywords</title>
        <kwd>Average precision</kwd>
        <kwd>Bayesian classifier</kwd>
        <kwd>Genomic selection</kwd>
        <kwd>Machine-learning algorithm</kwd>
        <kwd>Multi-layer perceptron</kwd>
        <kwd>Non-parametric model</kwd>
      </kwd-group>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; The Author(s) 2016</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1" sec-type="introduction">
      <title>Background</title>
      <p>Complex traits of economic importance in animal and plant breeding seem to be affected by many quantitative trait loci (QTL), each having a small effect, and are greatly influenced by the environment. Predicting these complex traits using information from dense molecular markers exploits linkage disequilibrium (LD) between molecular markers and QTL. Basically, genomic selection works by capturing realized relationships between individuals and, to an extent, by capturing the effects of QTL via their linkage or LD with markers. Genomic selection (GS) regression models use all available molecular marker and phenotypic data from an observed base (training population) to predict the genetic values of yet unphenotyped candidates for selection (testing population) whose marker genotypes are known.</p>
      <p>There is a vast literature describing statistical methods that use different functional forms on markers for predicting genetic values, e.g., [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>], starting with the seminal work of [<xref ref-type="bibr" rid="CR3">3</xref>], which proposed regressing phenotypes on all available markers using a Gaussian linear model with different prior distributions on marker effects. Several parametric and semi-parametric methods have been described and used thereafter for genome-enabled prediction in animals and plants [<xref ref-type="bibr" rid="CR4">4</xref>&#x2013;<xref ref-type="bibr" rid="CR11">11</xref>].</p>
      <p>The basic quantitative genetic model <italic>y</italic><sub><italic>i</italic></sub>&#x2009;=&#x2009;<italic>g</italic><sub><italic>i</italic></sub>&#x2009;+&#x2009;<italic>&#x3B3;</italic><sub><italic>i</italic></sub> (<italic>i</italic>&#x2009;=&#x2009;1,&#x2009;&#x2026;&#x2009;<italic>n</italic> individuals) describes the <italic>i</italic><sup><italic>th</italic></sup> response or phenotype (<italic>y</italic><sub><italic>i</italic></sub>) expressed as a deviation from some general mean (&#x3BC;) as the sum of an unknown genetic value (<italic>g</italic><sub><italic>i</italic></sub>) plus a model residual &#x3B3;<sub><italic>i</italic></sub>. The unknown genetic value can be represented as a complex function of genotypes with a large number of genes. However, since the genes affecting a trait are unknown, this complex function can be approximated by a regression of phenotype on marker genotypes where a large number of markers {<italic>x</italic><sub><italic>i</italic>1</sub>,&#x2009;&#x2026;,&#x2009;<italic>x</italic><sub><italic>ip</italic></sub>} (<italic>x</italic><sub><italic>ij</italic></sub> is the number of copies of one of the two alleles observed in the <italic>i</italic><sup><italic>th</italic></sup> individual at the <italic>j</italic><sup><italic>th</italic></sup> marker) may be used as regressors for predicting the genetic value of the <italic>i</italic><sup><italic>th</italic></sup> individual. Thus, for <italic>u</italic>(<bold>x</bold><sub><italic>i</italic></sub>)&#x2009;=&#x2009;<italic>u</italic>(<italic>x</italic><sub><italic>i</italic>1</sub>,&#x2009;&#x2026;&#x2009;<italic>x</italic><sub><italic>ip</italic></sub>), the basic model becomes <italic>y</italic><sub><italic>i</italic></sub>&#x2009;=&#x2009;<italic>u</italic><sub><italic>i</italic></sub>&#x2009;+&#x2009;<italic>&#x3B3;</italic><sub><italic>i</italic></sub>, where &#x3B3;<sub><italic>i</italic></sub> includes errors due to unspecified environmental effects, imperfect linkage disequilibrium between markers and the QTL affecting the trait, and unaccounted gene&#x2009;&#xD7;&#x2009;gene and gene&#x2009;&#xD7;&#x2009;environment interactions.</p>
      <p>In several applications, <italic>u</italic>(<bold>x</bold><sub><bold><italic>i</italic></bold></sub>) is a parametric linear regression with form <italic>u</italic>(<italic>x</italic><sub><italic>i</italic>1</sub>,&#x2009;&#x2026;&#x2009;<italic>x</italic><sub><italic>ip</italic></sub>)&#x2009;=&#x2009;&#x2211;<sub arrange="stack"><italic>j</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>p</italic></sup>&#x2009;
<italic>x</italic><sub><italic>ij</italic></sub><italic>&#x3B2;</italic><sub><italic>j</italic></sub>,, where &#x3B2;<sub><italic>j</italic></sub> is the substitution effect of the allele coded as &#x2018;one&#x2019; at the <italic>j</italic><sup><italic>th</italic></sup> marker. The linear regression function becomes <italic>y</italic><sub><italic>i</italic></sub>&#x2009;=&#x2009;&#x2211;<sub arrange="stack"><italic>j</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>p</italic></sup>&#x2009;
<italic>x</italic><sub><italic>ij</italic></sub><italic>&#x3B2;</italic><sub><italic>j</italic></sub>&#x2009;+&#x2009;<italic>&#x3B3;</italic><sub><italic>i</italic></sub>. The regression function <italic>u</italic>(<bold>x</bold><sub><italic>i</italic></sub>) can also be represented by semi-parametric models, such as reproducing kernel Hilbert space (RKHS) regressions or by different types of neural networks (NN) such as the multilayer perceptron or radial basis functions [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR11">11</xref>&#x2013;<xref ref-type="bibr" rid="CR14">14</xref>]. Several penalized linear regression models and Bayesian shrinkage estimation methods have been applied to genome-enabled prediction [<xref ref-type="bibr" rid="CR1">1</xref>]. Similarly, regularized machine learning has been used for predicting complex traits [<xref ref-type="bibr" rid="CR15">15</xref>]. Recently, two-layer feed-forward NN with backpropagation were implemented in various forms using German Fleckvieh and Holstein-Friesian bull data and high prediction accuracies were achieved [<xref ref-type="bibr" rid="CR16">16</xref>]. Likewise, a multi-layer NN classifier was applied to study genetic diversity in simulated experiments [<xref ref-type="bibr" rid="CR17">17</xref>].</p>
      <p>Nonparametric classification models are a branch of supervised machine learning that has been successfully applied in several fields of knowledge, e.g., text mining, bioinformatics and genomics [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. Particularly in applied genomic breeding programs and depending on the trait under consideration, the objective of classification is to focus on candidates for selection contained in the upper or lower classes of the prediction space. A common classification problem arises when an input marker vector <bold>x</bold><sub><italic>i</italic></sub>&#x2009;&#x2208;&#x2009;<italic>&#x211D;</italic><sup><italic>p</italic></sup> is to be assigned to one of <italic>S</italic> classes by a classifier. The classifier is trained using a set of training pairs (<bold>x</bold><sub><italic>i</italic></sub>,&#x2009;<italic>c</italic><sub><italic>i</italic></sub>),&#x2002;(<italic>i</italic>&#x2009;=&#x2009;1,&#x2009;&#x2026;&#x2009;<italic>n</italic>&#x2002;individuals), where <italic>c</italic><sub><italic>i</italic></sub> describes the class label (<italic>k</italic>) to which <bold>x</bold><sub><italic>i</italic></sub> belongs, (<italic>k</italic>&#x2009;=&#x2009;1&#x2009;&#x2026;&#x2009;<italic>S</italic>), where <italic>S</italic> represents the number of classes. Usually, <italic>c</italic><sub><italic>i</italic></sub> is transformed into a vector <bold>c</bold><sub><italic>i</italic></sub> of dimension <italic>S</italic>&#x2009;&#xD7;&#x2009;1, with 1 in class <italic>k</italic> and 0 otherwise.</p>
      <p>The multi-layer perceptron (MLP) classifier is a typical architecture of feed-forward NN with at least a hidden layer and an output layer, where both layers have nonlinear and differentiable transfer functions. The nonlinear transfer function in the hidden layer enables an NN to act as a universal approximation method. The training process of an MLP for each individual <italic>i</italic>, with input vector <bold>x</bold><sub><italic>i</italic></sub> and target class <bold>c</bold><sub><italic>i</italic></sub>, typically uses the error backpropagation learning algorithm [<xref ref-type="bibr" rid="CR20">20</xref>]. This process requires a lot of computational time when the number of input variables is large.</p>
      <p>The probabilistic neural network (PNN) was proposed by [<xref ref-type="bibr" rid="CR21">21</xref>] and is widely used in pattern recognition and classification. PNN classifies an input vector <bold>x</bold><sub><italic>i</italic></sub> into a specific <italic>k</italic> class such that the specific class has the maximum probability of being a correct assignment. PNN provides an optimum pattern classifier that minimizes the expected risk of wrongly classifying an object, and is a very efficient (in terms of computational time) classification method. The PNN training algorithm is simpler and faster than that of the MLP approach because PNN parameters are estimated directly from the input data and an iterative procedure is not required. Further, PNN guarantees convergence to a Bayes classifier if enough training examples are provided [<xref ref-type="bibr" rid="CR22">22</xref>]. Several classification methods such as support vector machines and random forests have been applied in GS [<xref ref-type="bibr" rid="CR23">23</xref>&#x2013;<xref ref-type="bibr" rid="CR25">25</xref>]. However, despite the apparent advantages of PNN, no PNN classifiers have been applied in GS so far.</p>
      <p>The objective of this research was to assess the performance of two NN classifiers, MLP and PNN (based on Gaussian kernels), to select individuals belonging to a specific class of interest (target class). In an applied GS context, the problem should be formulated according to whether the focus is on selecting individuals into the upper, middle or lower classes, depending on the trait under selection. Then the question is how many of the predicted individuals classified in the target class are actually observed in that class. The problem is posed as follows: given an input vector <bold>x</bold><sub><italic>i</italic></sub> of <italic>p</italic> markers for the <italic>i</italic><sup><italic>th</italic></sup> individual, each individual <italic>i</italic> in the testing set must be classified in a class of interest of the phenotypic response. Classes were defined considering different percentiles of the target trait, specifically, 15 and 30&#xA0;% for the upper and lower classes were analyzed.</p>
    </sec>
    <sec id="Sec2" sec-type="materials|methods">
      <title>Methods</title>
      <p>This section has four parts: first the two datasets are described; second, the strategy for categorizing the datasets is explained; third, the multilayer perceptron neural network (MLP) and probabilistic neural network (PNN) are described, and finally, the criteria used to assess model accuracy for classifying the best individuals based on genomic information are described.</p>
      <sec id="Sec3">
        <title>Maize datasets</title>
        <p>The maize datasets include 16 trait-environment combinations measured on 300 tropical lines genotyped with 55,000 SNPs each; these datasets were previously used by [<xref ref-type="bibr" rid="CR8">8</xref>]. Four datasets contain information on the complex trait grain yield (GY) evaluated under severe drought stress (GY-SS) and well-watered conditions (GY-WW), and in high yielding (GY-HI) and low yielding (GY-LO) environments. Another six datasets include information on days to anthesis or male flowering (MFL), on days to silking or female flowering (FFL), and the MFL to FFL interval (ASI) evaluated under severe drought stress (SS) and well-watered (WW) environments. The remaining six datasets contain information on gray leaf spot (GLS) resistance evaluated in six CIMMYT international trials (GLS-1 to GLS-6). The number of individuals and the type and number of markers are presented in Table&#xA0;<xref rid="Tab1" ref-type="table">1</xref>; for further details, see [<xref ref-type="bibr" rid="CR8">8</xref>].<table-wrap id="Tab1"><label>Table 1</label><caption><p>Maize datasets &#x2013; three classes</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th/><th/><th colspan="2">Number of individuals</th><th colspan="2">Number of individuals</th><th colspan="2">Number of individuals</th></tr><tr><th>Data set</th><th>Trait-environment combination</th><th>Number of SNP markers</th><th>Total number of individuals</th><th>Upper</th><th>Upper</th><th>Middle</th><th>Middle</th><th>Lower</th><th>Lower</th></tr><tr><th/><th/><th/><th/><th>15&#xA0;%</th><th>30&#xA0;%</th><th>40&#xA0;%</th><th>70&#xA0;%</th><th>15&#xA0;%</th><th>30&#xA0;%</th></tr></thead><tbody><tr><td>GY-HI</td><td>Yield in high yielding environment</td><td>46374</td><td>267</td><td>40</td><td>80</td><td>107</td><td>187</td><td>40</td><td>80</td></tr><tr><td>GY-LO</td><td>Yield in low yielding environment</td><td>46374</td><td>269</td><td>40</td><td>81</td><td>107</td><td>189</td><td>40</td><td>81</td></tr><tr><td>GY-WW</td><td>Yield in well watered</td><td>46374</td><td>242</td><td>36</td><td>73</td><td>96</td><td>170</td><td>36</td><td>73</td></tr><tr><td>GY-SS</td><td>Yield in drought stressed</td><td>46374</td><td>242</td><td>36</td><td>73</td><td>96</td><td>170</td><td>36</td><td>73</td></tr><tr><td>ASI-WW</td><td>Anthesis-silking interval in well watered</td><td>46374</td><td>258</td><td>39</td><td>79</td><td>102</td><td>180</td><td>39</td><td>77</td></tr><tr><td>ASI-SS</td><td>Anthesis-silking interval in drought stressed</td><td>46374</td><td>258</td><td>40</td><td>77</td><td>103</td><td>179</td><td>39</td><td>78</td></tr><tr><td>MFL-WW</td><td>Male flowering time in well watered</td><td>46374</td><td>258</td><td>40</td><td>139</td><td>103</td><td>178</td><td>40</td><td>78</td></tr><tr><td>MFL-SS</td><td>Male flowering time in drought stressed</td><td>46374</td><td>258</td><td>39</td><td>77</td><td>104</td><td>179</td><td>40</td><td>77</td></tr><tr><td>FFL-WW</td><td>Female flowering time in well watered</td><td>46374</td><td>258</td><td>39</td><td>77</td><td>104</td><td>179</td><td>40</td><td>77</td></tr><tr><td>FFL-SS</td><td>Female flowering time in drought stressed</td><td>46374</td><td>258</td><td>39</td><td>77</td><td>104</td><td>180</td><td>39</td><td>77</td></tr><tr><td>GLS-1</td><td>Gray leaf spot in environment 1</td><td>46374</td><td>272</td><td>42</td><td>87</td><td>68</td><td>170</td><td>60</td><td>117</td></tr><tr><td>GLS-2</td><td>Gray leaf spot in environment 2</td><td>46374</td><td>280</td><td>48</td><td>85</td><td>77</td><td>176</td><td>56</td><td>118</td></tr><tr><td>GLS-3</td><td>Gray leaf spot in environment 3</td><td>46374</td><td>278</td><td>47</td><td>85</td><td>107</td><td>168</td><td>63</td><td>86</td></tr><tr><td>GLS-4</td><td>Gray leaf spot in environment 4</td><td>46374</td><td>261</td><td>48</td><td>96</td><td>74</td><td>154</td><td>59</td><td>91</td></tr><tr><td>GLS-5</td><td>Gray leaf spot in environment 5</td><td>46374</td><td>279</td><td>48</td><td>97</td><td>84</td><td>188</td><td>43</td><td>98</td></tr><tr><td>GLS-6</td><td>Gray leaf spot in environment 6</td><td>46374</td><td>281</td><td>63</td><td>85</td><td>90</td><td>140</td><td>78</td><td>106</td></tr></tbody></table><table-wrap-foot><p>Trait&#x2013;environment combination, number of markers, total number of individuals, number of individuals in the upper 15 and 30&#xA0;% classes, in the middle 40 and 70&#xA0;% classes, and in the lower 15 and 30&#xA0;% classes from the empirical cumulative distribution function</p></table-wrap-foot></table-wrap></p>
      </sec>
      <sec id="Sec4">
        <title>Wheat datasets</title>
        <p>These datasets include 306 wheat lines from the CIMMYT Global Wheat Program (GWP) that were genotyped with 1717 Diversity Array Technology (DArT) markers generated by Triticarte Pty. Ltd. (Canberra, Australia; <ext-link ext-link-type="uri" xlink:href="http://www.diversityarrays.com">http://www.diversityarrays.com</ext-link>), which is a whole-genome profiling service laboratory. Two traits were analyzed, grain yield (GY) and days to heading (DTH), which were evaluated in different environments (year-drought stress-agronomic treatments). GY was measured in seven environments and DTH in ten environments. The number of individuals and the type and number of markers are presented in Table&#xA0;<xref rid="Tab2" ref-type="table">2</xref>; for further details, see [<xref ref-type="bibr" rid="CR11">11</xref>].<table-wrap id="Tab2"><label>Table 2</label><caption><p>Wheat datasets &#x2013; three classes</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th/><th/><th/><th/><th colspan="2">Number of individuals</th><th colspan="2">Number of individuals</th><th colspan="2">Number of individuals</th></tr><tr><th>Data set</th><th>Agronomic management</th><th>Site in Mexico</th><th>Year</th><th>Number of SNP markers</th><th>Total number of individuals</th><th>Upper</th><th>Upper</th><th>Middle</th><th>Middle</th><th>Lower</th><th>Lower</th></tr><tr><th/><th/><th/><th/><th/><th/><th>15&#xA0;%</th><th>30&#xA0;%</th><th>40&#xA0;%</th><th>70&#xA0;%</th><th>15&#xA0;%</th><th>30&#xA0;%</th></tr></thead><tbody><tr><td>GY-1</td><td>Drought-bed</td><td>Cd. Obregon</td><td>2009</td><td>1717</td><td>306</td><td>46</td><td>92</td><td>119</td><td>211</td><td>49</td><td>95</td></tr><tr><td>GY-2</td><td>Drought-bed</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>306</td><td>47</td><td>92</td><td>122</td><td>213</td><td>46</td><td>92</td></tr><tr><td>GY-3</td><td>Drought-flat</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>263</td><td>39</td><td>80</td><td>104</td><td>185</td><td>39</td><td>79</td></tr><tr><td>GY-4</td><td>Full irrigation-bed</td><td>Cd. Obregon</td><td>2009</td><td>1717</td><td>304</td><td>46</td><td>92</td><td>120</td><td>212</td><td>46</td><td>92</td></tr><tr><td>GY-5</td><td>Full irrigation-bed</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>306</td><td>46</td><td>94</td><td>118</td><td>214</td><td>46</td><td>94</td></tr><tr><td>GY-6</td><td>Heat-bed</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>306</td><td>46</td><td>94</td><td>120</td><td>214</td><td>46</td><td>92</td></tr><tr><td>GY-7</td><td>Full irrigation-flat</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>263</td><td>39</td><td>79</td><td>105</td><td>185</td><td>39</td><td>79</td></tr><tr><td>DTH-1</td><td>Drought-bed</td><td>Cd. Obregon</td><td>2009</td><td>1717</td><td>306</td><td>53</td><td>100</td><td>93</td><td>197</td><td>56</td><td>113</td></tr><tr><td>DTH-2</td><td>Drought-bed</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>306</td><td>50</td><td>93</td><td>117</td><td>198</td><td>58</td><td>96</td></tr><tr><td>DTH-3</td><td>Drought-flat</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>263</td><td>40</td><td>86</td><td>77</td><td>177</td><td>46</td><td>100</td></tr><tr><td>DTH-4</td><td>Full irrigation-bed</td><td>Cd. Obregon</td><td>2009</td><td>1717</td><td>306</td><td>59</td><td>107</td><td>107</td><td>173</td><td>74</td><td>92</td></tr><tr><td>DTH-5</td><td>Full irrigation-bed</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>306</td><td>47</td><td>101</td><td>105</td><td>207</td><td>52</td><td>100</td></tr><tr><td>DTH-6</td><td>Toluca</td><td>Toluca</td><td>2009</td><td>1717</td><td>306</td><td>122</td><td>122</td><td>75</td><td>93</td><td>91</td><td>109</td></tr><tr><td>DTH-7</td><td>El Batan</td><td>El Batan</td><td>2009</td><td>1717</td><td>306</td><td>66</td><td>104</td><td>101</td><td>175</td><td>65</td><td>101</td></tr><tr><td>DTH-8</td><td>Small observation plot</td><td>Cd. Obregon</td><td>2009</td><td>1717</td><td>301</td><td>58</td><td>101</td><td>100</td><td>182</td><td>61</td><td>100</td></tr><tr><td>DTH-9</td><td>Small observation plot</td><td>Cd. Obregon</td><td>2010</td><td>1717</td><td>263</td><td>45</td><td>100</td><td>76</td><td>173</td><td>45</td><td>87</td></tr><tr><td>DTH-10</td><td>Agua Fria</td><td>Agua Fria</td><td>2010</td><td>1717</td><td>261</td><td>49</td><td>81</td><td>93</td><td>125</td><td>87</td><td>87</td></tr></tbody></table><table-wrap-foot><p>Environment code of 12 combinations of sites in Mexico, agronomic management, and year for two wheat traits (grain yield, GY, and days to heading, DTH) from [<xref ref-type="bibr" rid="CR11">11</xref>]. Number of markers, total number of individuals, number of individuals in the upper 15 and 30&#xA0;% classes, in the middle 40 and 70&#xA0;% classes, and in the lower 15 and 30&#xA0;% classes from the empirical cumulative distribution</p></table-wrap-foot></table-wrap></p>
      </sec>
      <sec id="Sec5">
        <title>Transforming phenotypic responses into three or two classes</title>
        <p>The continuous phenotypic responses <italic>y</italic><sub><italic>i</italic></sub> for each stratified random partition in the datasets were grouped into three classes (upper, middle and lower), based on 15&#x2013;85&#xA0;% and 30&#x2013;70&#xA0;% percentiles of the response of each trait analyzed. For example, for 15&#x2013;85&#xA0;% percentiles, the quantiles <italic>q</italic><sub>0.15</sub> and <italic>q</italic><sub>0.85</sub> were used to split <italic>y</italic><sub><italic>i</italic></sub> into three classes: <italic>y</italic><sub><italic>i</italic></sub>&#x2009;&#x2208; upper class, if <italic>y</italic><sub><italic>i</italic></sub>&#x2009;&gt;&#x2009;<italic>q</italic><sub>0.85</sub>; <italic>y</italic><sub><italic>i</italic></sub>&#x2009;&#x2208; middle class, if <italic>q</italic><sub>0.15</sub>&#x2009;&lt;&#x2009;<italic>y</italic><sub><italic>i</italic></sub>&#x2009;&#x2264;&#x2009;<italic>q</italic><sub>0.85</sub>; and, <italic>y</italic><sub><italic>i</italic></sub>&#x2009;&#x2208; lower class; <italic>if y</italic><sub><italic>i</italic></sub>&#x2009;&#x2264;&#x2009;<italic>q</italic><sub>0.15</sub> A similar rule was applied to split y<sub><italic>i</italic></sub> into three classes with 30-70&#xA0;% percentiles.</p>
        <p>For the two species, the target classes were the upper 15 and 30&#xA0;% classes (GY for maize and wheat); the middle 40 and 70&#xA0;% classes (ASI for maize), and the lower 15 and 30&#xA0;% classes (FFL, MFL, and GLS for maize and DTH for wheat).</p>
        <p>Comparison of prediction accuracy of PNN based on two or three classes was performed only on the wheat datasets to simplify computations. Firstly, the phenotypic responses <italic>y</italic><sub><italic>i</italic></sub> for each stratified random partition of the wheat datasets were grouped into two classes from the datasets previously grouped into three classes. The upper 15&#xA0;% of the binary class was defined by using the upper 15&#xA0;% of the trichotomous classes, and the lower class was the sum of the middle and lower classes of the trichotomous classes; a similar strategy was applied for the lower 15&#xA0;% of the binary class. The same random partitions (training, testing sets) were used when comparing PNN with two classes versus PNN with three classes. Partitions of the wheat datasets into two classes for GY and DTH are shown in Table&#xA0;<xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Wheat datasets &#x2013; two classes</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th/><th/><th colspan="2">Number of individuals</th><th colspan="2">Number of individuals</th></tr><tr><th>Data set</th><th>Agronomic management</th><th>Site in Mexico</th><th>Year</th><th>Upper</th><th>Lower</th><th>Upper</th><th>Lower</th></tr><tr><th/><th/><th/><th/><th>15&#xA0;%</th><th>85&#xA0;%</th><th>30&#xA0;%</th><th>70&#xA0;%</th></tr></thead><tbody><tr><td>GY-1</td><td>Drought-bed</td><td>Cd. Obregon</td><td>2009</td><td>46</td><td>260</td><td>92</td><td>214</td></tr><tr><td>GY-2</td><td>Drought-bed</td><td>Cd. Obregon</td><td>2010</td><td>47</td><td>259</td><td>92</td><td>214</td></tr><tr><td>GY-3</td><td>Drought-flat</td><td>Cd. Obregon</td><td>2010</td><td>39</td><td>224</td><td>80</td><td>183</td></tr><tr><td>GY-4</td><td>Full irrigation-bed</td><td>Cd. Obregon</td><td>2009</td><td>46</td><td>258</td><td>92</td><td>212</td></tr><tr><td>GY-5</td><td>Full irrigation-bed</td><td>Cd. Obregon</td><td>2010</td><td>46</td><td>260</td><td>94</td><td>212</td></tr><tr><td>GY-6</td><td>Heat-bed</td><td>Cd. Obregon</td><td>2010</td><td>46</td><td>260</td><td>94</td><td>212</td></tr><tr><td>GY-7</td><td>Full irrigation-flat-borders</td><td>Cd. Obregon</td><td>2010</td><td>39</td><td>224</td><td>79</td><td>184</td></tr><tr><td/><td/><td/><td/><td>Lower</td><td>Upper</td><td>Lower</td><td>Upper</td></tr><tr><td/><td/><td/><td/><td>15&#xA0;%</td><td>85&#xA0;%</td><td>30&#xA0;%</td><td>70&#xA0;%</td></tr><tr><td>DTH-1</td><td>Drought-bed</td><td>Cd. Obregon</td><td>2009</td><td>53</td><td>253</td><td>100</td><td>206</td></tr><tr><td>DTH-2</td><td>Drought-bed</td><td>Cd. Obregon</td><td>2010</td><td>50</td><td>256</td><td>93</td><td>213</td></tr><tr><td>DTH-3</td><td>Drought-flat</td><td>Cd. Obregon</td><td>2010</td><td>40</td><td>223</td><td>86</td><td>177</td></tr><tr><td>DTH-4</td><td>Full irrigation-bed</td><td>Cd. Obregon</td><td>2009</td><td>59</td><td>247</td><td>107</td><td>199</td></tr><tr><td>DTH-5</td><td>Full irrigation-bed</td><td>Cd. Obregon</td><td>2010</td><td>47</td><td>259</td><td>101</td><td>205</td></tr><tr><td>DTH-6</td><td>Toluca</td><td>Toluca</td><td>2009</td><td>122</td><td>184</td><td>122</td><td>184</td></tr><tr><td>DTH-7</td><td>El Batan</td><td>El Batan</td><td>2009</td><td>66</td><td>240</td><td>104</td><td>202</td></tr><tr><td>DTH-8</td><td>Small observation plot</td><td>Cd. Obregon</td><td>2009</td><td>58</td><td>243</td><td>101</td><td>200</td></tr><tr><td>DTH-9</td><td>Small observation plot</td><td>Cd. Obregon</td><td>2010</td><td>45</td><td>218</td><td>100</td><td>163</td></tr><tr><td>DTH-10</td><td>Agua Fria</td><td>Agua Fria</td><td>2010</td><td>49</td><td>212</td><td>81</td><td>160</td></tr></tbody></table><table-wrap-foot><p>Environment code of 12 combinations of sites in Mexico, agronomic management, and year for two wheat traits (grain yield, GY, and days to heading, DTH) from [<xref ref-type="bibr" rid="CR11">11</xref>]. Number of markers, total number of individuals, number of individuals in the upper 15 and 30&#xA0;% classes, and in the lower 85 and 70&#xA0;% classes</p></table-wrap-foot></table-wrap></p>
      </sec>
      <sec id="Sec6">
        <title>Multilayer perceptron neural network (MLP) classifier</title>
        <p>An MLP can be trained to classify items into <italic>S</italic> different disjoint classes. Each target class <italic>c</italic><sub><italic>i</italic></sub> is transformed into a target vector <bold><italic>c</italic></bold><sub><italic>i</italic></sub> of zeroes except for a 1 in element <italic>k</italic>, (<italic>k</italic>&#x2009;=&#x2009;1,&#x2009;&#x2026;,&#x2009;<italic>S</italic>) the class to be represented. We arranged a set of <italic>n</italic> input vectors <bold>x</bold><sub><italic>i</italic></sub> into a matrix <bold>X</bold> of dimension <italic>n</italic>&#x2009;&#xD7;&#x2009;<italic>p</italic>. Then we arranged the <italic>n</italic> target vectors <italic>c</italic><sub><italic>i</italic></sub> into a matrix <bold>C</bold> of dimension <italic>S</italic>&#x2009;&#xD7;&#x2009;<italic>n</italic>. The rows of <bold>X</bold> correspond to columns of <bold>C</bold>, individual-by-individual. Statistical learning is inferred from the data only, with no assumption about the joint distribution of inputs and outcomes. This gives MLP great flexibility for capturing complex patterns frequently found in plant breeding [<xref ref-type="bibr" rid="CR26">26</xref>].</p>
        <p>We begin by describing a standard MLP for a categorical response (PNN is introduced subsequently). MLP is an NN that can be thought of as a two-stage regression (e.g., [<xref ref-type="bibr" rid="CR18">18</xref>]). In the first stage (hidden layer), <italic>M</italic> data-derived basis functions, {<italic>z</italic><sub><italic>m</italic></sub>}<sub arrange="stack"><italic>m</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>m</italic>&#x2009;=&#x2009;<italic>M</italic></sup> are inferred; in the second stage (the output layer has <italic>S</italic> neurons, <italic>S</italic> classes), each neuron&#x2019;s output is computed on the basis functions inferred in the hidden layer using a nonlinear transfer function (Fig.&#xA0;<xref rid="Fig1" ref-type="fig">1</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><p>Architecture of classifier MLP with the input (markers) layer, hidden layer, and sum-output layer</p></caption><graphic xlink:href="12864_2016_2553_Fig1_HTML" id="MO1"/></fig></p>
        <p>In the hidden layer, one data-derived predictor is inferred at each of <italic>M</italic> neurons. These data-derived predictors are computed by first inferring a score (<italic>u</italic><sub><italic>mi</italic></sub>), which is a linear combination of the input weights and the input markers plus a bias (intercept) term. Subsequently, this score is transformed using a nonlinear transfer function, <italic>&#x3C6;</italic>(&#x22C5;), that is, <italic>z</italic><sub><italic>mi</italic></sub>&#x2009;=&#x2009;<italic>&#x3C6;</italic>(<italic>w</italic><sub><italic>mo</italic></sub>&#x2009;+&#x2009;&#x2211;<sub arrange="stack"><italic>j</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>p</italic></sup><italic>w</italic><sub><italic>mj</italic></sub><italic>x</italic><sub><italic>ij</italic></sub>), where <italic>w</italic><sub><italic>mo</italic></sub> is the bias term, and <italic>W</italic><sub><italic>m</italic></sub>&#x2009;=&#x2009;{<italic>W</italic><sub><italic>mj</italic></sub>}<sub arrange="stack"><italic>m</italic>&#x2009;=&#x2009;1;&#x2002;<italic>j</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>m</italic>&#x2009;=&#x2009;<italic>M</italic>;&#x2002;<italic>j</italic>&#x2009;=&#x2009;<italic>p</italic></sup> is an input weight matrix. The transfer function maps from a score defined in the real line onto the interval [&#x2212;1, 1] (e.g., a hyperbolic tangent sigmoid transfer function is <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ tansig(u)=\frac{2}{\left(\left(1+ exp\left(-2\right)\right)-1\right)} $$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="italic">tansig</mml:mi><mml:mfenced close=")" open="("><mml:mi>u</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mfenced close=")" open="("><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="italic">exp</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mo>&#x2212;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>&#x2212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mfrac></mml:math><inline-graphic xlink:href="12864_2016_2553_Article_IEq1.gif"/></alternatives></inline-formula>. Subsequently, in the output layer, phenotypes are regressed on the data-derived features, {<italic>z</italic><sub><italic>mi</italic></sub>}<sub arrange="stack"><italic>i</italic>&#x2009;=&#x2009;1;&#x2002;<italic>m</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>i</italic>&#x2009;=&#x2009;<italic>n</italic>;&#x2002;<italic>m</italic>&#x2009;=&#x2009;<italic>M</italic></sup>, according to the model E(<italic>y</italic><sub><italic>ki</italic></sub>|<italic>parameters</italic>)&#x2009;=&#x2009;<italic>v</italic><sub><italic>ki</italic></sub>&#x2009;=&#x2009;<italic>w</italic><sub><italic>ko</italic></sub>&#x2009;+&#x2009;&#x2211;<sub arrange="stack"><italic>m</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>M</italic></sup><italic>w</italic><sub><italic>mi</italic></sub>, where <italic>&#x3C6;</italic><sub><italic>k</italic></sub>(<italic>v</italic><sub><italic>ki</italic></sub>) and <italic>&#x3C6;</italic><sub><italic>k</italic></sub>(.) is the <italic>tansig</italic> transfer function, <italic>k</italic>&#x2009;=&#x2009;1,&#x2009;&#x2026;,&#x2009;<italic>S</italic>. Finally, the predicted score vector <bold>&#x109;</bold><sub><italic>i</italic></sub>&#x2009;=&#x2009;{<italic>y</italic><sub><italic>ki</italic></sub>}<sub arrange="stack"><italic>k</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>k</italic>&#x2009;=&#x2009;<italic>S</italic></sup>, and the predicted class <italic>&#x109;</italic><sub><italic>i</italic></sub> is <italic>&#x109;</italic>(<bold>x</bold><sub><italic>i</italic></sub>)&#x2009;=&#x2002;arg&#x2002;<italic>max</italic><sub>1&#x2009;&#x2264;&#x2009;<italic>k</italic>&#x2009;&#x2264;&#x2009;<italic>S</italic></sub>(<italic>&#x109;</italic><sub><italic>k</italic></sub>) are obtained.</p>
        <p>Training of an MLP (given a fixed number of transfer functions in the hidden layer) involves estimating all of the classifier&#x2019;s parameters by means of an iterative backpropagation error algorithm, based on the scaled conjugate gradient algorithm described by [<xref ref-type="bibr" rid="CR27">27</xref>]. To improve the generalization capacity of MLP, an early stopping ensemble strategy can be applied [<xref ref-type="bibr" rid="CR28">28</xref>]; early stopping effects non-Bayesian shrinkage of coefficients. In this approach, we divided the available data into three subsets. The first subset is the training set, used for computing the gradient and updating network weights and biases. The second subset is the validation set, where the error in the set is monitored during the training process. The validation error normally decreases during the initial training phase, as does the training set error. However, when the network begins to over-fit the data, the error in the validation set typically begins to rise. When the validation error increases at some point in the iteration, the training is stopped, and the weights and biases at the minimum validation error are returned. The third subset is used as testing set.</p>
        <p>The performance function to optimize an MLP is usually the mean squared error (<italic>mse</italic>), which is the average squared error between the predicted classes <bold>&#x108;</bold> and the target classes <bold>C. &#x108;</bold> is also a matrix of dimension <italic>S</italic>&#x2009;&#xD7;&#x2009;<italic>n</italic>, where each column contains values in the [0,1] range. The index of the largest element in the column indicates which of the <italic>S</italic> classes that vector represents.</p>
      </sec>
      <sec id="Sec7">
        <title>Probabilistic neural network (PNN) classifier</title>
        <p>The architecture of a PNN is similar to that of a radial basis function NN [<xref ref-type="bibr" rid="CR8">8</xref>]; a PNN has two layers, the pattern layer and the summation-output layer, as illustrated in Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2</xref>. The pattern layer computes distances (using a Gaussian radial basis function (RBF)) between the input vector <bold>x</bold><sub><italic>i</italic></sub> and the training (centers) input vectors <bold>c</bold><sub><italic>m</italic></sub>&#x2009;&#x2208;&#x2009;<italic>&#x211D;</italic><sup><italic>p</italic></sup>;&#x2002;<italic>m</italic>&#x2009;=&#x2009;1,&#x2009;&#x2026;,&#x2009;<italic>M</italic> neurons (<italic>M&#x2009;=&#x2009;n</italic> individuals of the input data set) and returns an output vector <bold>u</bold><sub><italic>i</italic></sub>&#x2009;&#x2208;&#x2009;<italic>&#x211D;</italic><sup><italic>M</italic></sup> whose elements <italic>u</italic><sub><italic>mi</italic></sub>&#x2009;=&#x2009;<italic>b</italic><sub><italic>m</italic></sub>&#x2016;<bold>x</bold><sub><italic>i</italic></sub>&#x2009;&#x2212;&#x2009;<bold><italic>c</italic></bold><sub><italic>m</italic></sub>&#x2016;, where <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {b}_m=\frac{\sqrt{\left(-\mathrm{In}(0.5)\right)}}{h} $$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>b</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:mfenced close=")" open="("><mml:mrow><mml:mo>&#x2212;</mml:mo><mml:mi mathvariant="normal">In</mml:mi><mml:mfenced close=")" open="("><mml:mn>0.5</mml:mn></mml:mfenced></mml:mrow></mml:mfenced></mml:msqrt><mml:mi>h</mml:mi></mml:mfrac></mml:math><inline-graphic xlink:href="12864_2016_2553_Article_IEq2.gif"/></alternatives></inline-formula> is a weight and <italic>h</italic> is the width of the Gaussian RBF, indicating how close the input vector <bold>x</bold><sub><italic>i</italic></sub> is to <bold>c</bold><sub><italic>m</italic></sub> [<xref ref-type="bibr" rid="CR22">22</xref>]. Then each <italic>u</italic><sub><italic>mi</italic></sub> is transformed into a vector <bold>z</bold><sub><italic>i</italic></sub>&#x2009;&#x2208;&#x2009;<italic>&#x211D;</italic><sup><italic>M</italic></sup>, whose elements are defined by the Gaussian operation <italic>z</italic><sub><italic>mi</italic></sub>&#x2009;=&#x2009;<italic>exp</italic>(&#x2212;<italic>u</italic><sub arrange="stack"><italic>mi</italic></sub><sup arrange="stack">2</sup>). The summation-output layer sums these contributions for each class <italic>k</italic>, that is, <italic>v</italic><sub><italic>ki</italic></sub>&#x2009;=&#x2009;&#x2211;<sub arrange="stack"><italic>m</italic>&#x2009;=&#x2009;1</sub><sup arrange="stack"><italic>M</italic></sup><italic>w</italic><sub><italic>km</italic></sub><italic>z</italic><sub><italic>mi</italic></sub>, where <italic>w</italic><sub><italic>km</italic></sub> are weights obtained from the target classes <bold>C</bold> matrix, to generate a vector of probabilities <bold>&#x109;</bold><sub><italic>i</italic></sub>&#x2009;=&#x2009;<italic>softmax</italic>(<bold>v</bold><sub><italic>i</italic></sub>) of dimension <italic>S</italic>&#x2009;&#xD7;&#x2009;1 as its net output, where the <italic>softmax</italic> transfer function <italic>&#x3C3;</italic>(.) is given by<fig id="Fig2"><label>Fig. 2</label><caption><p>Architecture of classifier PNN with the input (markers) layer, pattern layer, and sum-output layer</p></caption><graphic xlink:href="12864_2016_2553_Fig2_HTML" id="MO2"/></fig><disp-formula id="Equa"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \sigma \left({\mathbf{v}}_i\right)=\frac{exp\left({v}_k\right)}{{\displaystyle {\sum}_{j=1}^S exp\left({v}_j\right)}};\kern0.5em for\kern0.5em k=1,\dots, S\kern0.5em \mathrm{classes} $$\end{document}</tex-math><mml:math id="M6"><mml:mi>&#x3C3;</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo stretchy="true">&#x2211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>S</mml:mi></mml:msubsup><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mstyle></mml:mfrac><mml:mo>;</mml:mo><mml:mspace width="0.5em"/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.5em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x2026;</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mspace width="0.5em"/><mml:mi mathvariant="normal">classes</mml:mi></mml:math><graphic xlink:href="12864_2016_2553_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <bold>v</bold><sub><italic>i</italic></sub> is a target vector of dimension <italic>S</italic>&#x2009;&#xD7;&#x2009;1 with elements <italic>v</italic><sub><italic>k</italic></sub>. The <italic>softmax</italic> transfer function on the summation-output layer transforms the outputs of processing units for each <italic>k</italic> class in the interval [0,1].</p>
        <p>The pattern layer of a PNN is a neural representation of a Bayes classifier, where the class density functions are approximated using a windows Parzen estimator [<xref ref-type="bibr" rid="CR29">29</xref>]. The standard training method for a PNN (given a value of <italic>h</italic> for the Gaussian RBFs) requires a single pass over all the <bold>x</bold><sub><italic>i</italic></sub> markers of the training set. For this reason, PNN requires short training time and produce as output (<bold><italic>&#x109;</italic></bold><sub><italic>i</italic></sub>), posterior probabilities of class membership.</p>
      </sec>
      <sec id="Sec8">
        <title>Criteria for assessing classifier prediction accuracy</title>
        <p>The prediction accuracy of MLP and PNN was evaluated using a cross-validation procedure. For each data set, 50 random partitions stratified by classes were generated. Each partition randomly assigned 90&#xA0;% of the data to the training set and the remaining 10&#xA0;% to the testing set. We used stratified sampling by class to make sure there were no empty classes in the training and testing sets. For each data set, partition index matrices PINDX(<italic>n</italic>, 50) were generated, where <italic>n</italic> is the number of individuals in each data set analyzed; PINDX(i,j) has a value equal to 1 (training) or 2 (testing) for the <italic>i</italic><sup>th</sup> individual in the <italic>j</italic><sup>th</sup> partition. Each model was trained and evaluated with the same pair of training and testing sets of each partition. For MLP the training sets defined in PINDX(<italic>n</italic>, 50) were subdivided by stratified random sampling by class into two disjoint sets, one for training (88&#xA0;%) and another for validation (12&#xA0;%); this was done with the objective of applying the training early stopping ensemble strategy[<xref ref-type="bibr" rid="CR28">28</xref>]. For each random partition, ten replications (random seeds) were used to evaluate the performance of MLP.</p>
        <p>Two performance measures for assessing prediction accuracy of the two classifiers (averaged across 50 random partitions) were used: (1) the area under the receiver operating characteristic curve (<italic>AUC</italic>), and (2) the area under the precision-recall curve (<italic>AUCpr</italic>), or average precision.</p>
        <p>For GY in both species, models were trained to maximize the <italic>AUC</italic> of the upper class; for FL, GLS, and DTH, models were trained to maximize the <italic>AUC</italic> of the lower class; for ASI, the target value is zero (perfect synchrony between anthesis and silking interval), models were trained to maximize the <italic>AUC</italic> of the middle class.</p>
      </sec>
      <sec id="Sec9">
        <title>The area under the receiver operating characteristic curve (<italic>AUC</italic>)</title>
        <p>Rather than computing the recall (<italic>R</italic>) [also called sensitivity or true positive rate (<italic>tpr</italic>)] and the false positive rate (<italic>fpr</italic>) for a fixed threshold &#x3C4;, a set of thresholds was defined and then <italic>tpr</italic>&#x2002;vs&#x2002;<italic>fpr</italic>(<italic>R</italic>&#x2002;vs&#x2002;<italic>f&#x2002;pr</italic>) was plotted as an implicit function of &#x3C4;; this is called an ROC curve.</p>
        <p>The recall or sensitivity is <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ R=\frac{tp}{tp+fn}, $$\end{document}</tex-math><mml:math id="M8"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mtext>,</mml:mtext></mml:math><inline-graphic xlink:href="12864_2016_2553_Article_IEq3.gif"/></alternatives></inline-formula> where <italic>tp</italic> is the number of positives predicted as positives and <italic>fn</italic> is the number of positives predicted as negatives. This measure evaluates the number of individuals that are correctly classified as a proportion of all the observed individuals in the target class. <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ fpr=\frac{fp}{fp+tn}, $$\end{document}</tex-math><mml:math id="M10"><mml:mi>f</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mtext>,</mml:mtext></mml:math><inline-graphic xlink:href="12864_2016_2553_Article_IEq4.gif"/></alternatives></inline-formula> where <italic>fp</italic> is the number of negatives predicted as positives and <italic>tn</italic> is the number of negatives predicted as negatives (Table&#xA0;<xref rid="Tab4" ref-type="table">4</xref>).<table-wrap id="Tab4"><label>Table 4</label><caption><p>Description of a confusion matrix for binary classes with observed values and classifier predicted values</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="2"/><th colspan="2">Classifier predicted value</th><th rowspan="2">Sum</th></tr><tr><th>1</th><th>0</th></tr></thead><tbody><tr><td rowspan="2">Observed</td><td>1</td><td><italic>tp</italic></td><td><italic>fn</italic> (Type II error)</td><td><italic>tp&#x2009;+&#x2009;fn</italic></td></tr><tr><td>0</td><td><italic>fp</italic> (Type I error)</td><td><italic>tn</italic></td><td><italic>fp</italic>&#x2009;+&#x2009;<italic>tn</italic></td></tr><tr><td colspan="2">Sum</td><td><italic>tp</italic>&#x2009;+&#x2009;<italic>fp</italic></td><td><italic>fn</italic>&#x2009;+&#x2009;<italic>tn</italic></td><td><italic>n</italic></td></tr></tbody></table><table-wrap-foot><p><italic>tp</italic> true positive, <italic>fp</italic> false positive, <italic>fn</italic> false negative, <italic>tn</italic> true negative, <italic>n</italic> total number of individuals</p></table-wrap-foot></table-wrap></p>
        <p>To compare the performance of classifiers, the receiver operating characteristic curve <bold>(</bold>ROC) has to be reduced to a single scalar value representing the expected performance. A common method is to compute the area under the ROC curve (<italic>AUC</italic>), which produces a value between 0 and 1. If <italic>AUC</italic>(<italic>a</italic>)&#x2009;&gt;&#x2002;<italic>AUC</italic>(<italic>b</italic>), then classifier <italic>a</italic> has a better average performance than classifier <italic>b. AUC</italic> can be interpreted as the probability that a randomly chosen individual is ranked as more likely to be of the target class than a randomly chosen individual of another class. The ROC graphs are a useful tool for visualizing the performance of the classifiers because they provide a richer measure of classification performance than other scalar measures [<xref ref-type="bibr" rid="CR30">30</xref>].</p>
      </sec>
      <sec id="Sec10">
        <title>The area under the precision-recall curve (<italic>AUCpr</italic>)</title>
        <p>A precision-recall curve is a plot of precision (<italic>P</italic>) vs <italic>R</italic> for a set of thresholds &#x3C4;. <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ P=\frac{tp}{tp+fp} $$\end{document}</tex-math><mml:math id="M12"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12864_2016_2553_Article_IEq5.gif"/></alternatives></inline-formula> is defined as the fraction of positives predicted as positives with respect to all predicted positives (Table&#xA0;<xref rid="Tab4" ref-type="table">4</xref>). Thus <italic>P</italic> measures the fraction of the predicted positives that is really positive, while <italic>R</italic> measures the fraction of the predictive positives that was in fact detected. This curve is summarized as a single number using the average precision (<italic>AUCpr</italic>), which approximates the area under the precision-recall curve [<xref ref-type="bibr" rid="CR31">31</xref>]. This measure is recommended for classes of different sizes; upper or lower classes of 15&#xA0;% had a lower number of individuals than the corresponding upper or lower classes of 85&#xA0;%. <italic>AUC</italic> is commonly used to present results of binary decision problems in machine learning algorithms. However, when dealing with unbalanced classes, <italic>AUCpr</italic> curves give a more informative idea of a machine learning algorithm than <italic>AUC</italic> [<xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR33">33</xref>].</p>
      </sec>
      <sec id="Sec11">
        <title>Software</title>
        <p>Scripts for fitting models and performing cross-validations were written in MATLAB r2010b. All the analyses were performed in a Linux Workstation.</p>
      </sec>
    </sec>
    <sec id="Sec12" sec-type="results">
      <title>Results and discussion</title>
      <p>Results of the value of <italic>AUC</italic> for classifiers MLP and PNN in each trait-environment combination are depicted in histograms in Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3a</xref>&#x2013;<xref rid="Fig3" ref-type="fig">d</xref> (maize datasets) and Fig.&#xA0;<xref rid="Fig4" ref-type="fig">4a</xref>&#x2013;<xref rid="Fig4" ref-type="fig">b</xref> (wheat datasets) for the traits selected in the upper and lower (15 and 30&#xA0;%) and middle (40 and 70&#xA0;%) classes, respectively.<fig id="Fig3"><label>Fig. 3</label><caption><p>Histograms of the <italic>AUC</italic> criterion and their standard deviation (error bars) for the maize datasets. <bold>a</bold> grain yield (GY) under optimal conditions (HI and WW) and stress conditions (LO and SS) of classifiers MLP and PNN in the upper 15 and 30&#xA0;% classes; <bold>b</bold> anthesis-silking interval (ASI) under optimal conditions (WW) and drought stress conditions (SS) of MLP and PNN of the middle 40 and 70&#xA0;% classes; <bold>c</bold> female flowering time (FFL), male flowering time (MFL) under optimal well-watered (WW) conditions and drought stress conditions (SS) of MLP and PNN of the lower 15 and 30&#xA0;% classes; <bold>d</bold> gray leaf spot resistance (GLS) in 6 environments (1&#x2013;6) of MLP and PNN of the lower 15 and 30&#xA0;% classes</p></caption><graphic xlink:href="12864_2016_2553_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Histograms of the <italic>AUC</italic> criterion and their standard deviation (error bars) for the wheat datasets. <bold>a</bold> grain yield (GY) in seven environments (1&#x2013;7) of classifiers MLP and PNN of the upper 15 and 30&#xA0;% classes; <bold>b</bold> days to heading (DTH) in ten environments (1&#x2013;10) of MLP and PNN in the lower 15 and 30&#xA0;% classes</p></caption><graphic xlink:href="12864_2016_2553_Fig4_HTML" id="MO4"/></fig></p>
      <p>The first clear trend using the <italic>AUC</italic> criterion is that PNN outperformed MLP for most of the individuals in the upper, middle and lower classes. Depending on the trait-environment combination, the PNN30% or PNN15% upper and lower and the PNN40% and PNN70% middle were usually larger than those of MLP; the only exception was PNN15% for GY-SS (Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3a</xref>), which was lower than MLP15% (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1).</p>
      <p>We also describe <italic>AUC</italic> and <italic>AUCpr</italic> results of comparing the performance of PNN for wheat trait-environment combinations using two or three classes.</p>
      <sec id="Sec13">
        <title>Comparing classifiers to select individuals in the upper, middle and lower classes in the maize datasets</title>
        <sec id="Sec14">
          <title>Upper classes (15 and 30&#xA0;%)</title>
          <p>Results of the prediction accuracy criterion <italic>AUC</italic> of the two classifiers MLP and PNN for traits selected in the 15 and 30&#xA0;% upper classes for GY under the different environmental conditions are reported in Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3a</xref>. PNN was more accurate than MLP in the upper 30&#xA0;% class, for assigning individuals based on GY under stress conditions. Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1 shows the results based on the <italic>AUC</italic> criterion for the upper, middle and lower classes.</p>
          <p>When using the <italic>AUCpr</italic> criterion, which relates <italic>P</italic> and <italic>R</italic> for the upper class, PNN outperformed MLP, which is clearly shown in Table&#xA0;<xref rid="Tab5" ref-type="table">5</xref> (as shown for the <italic>AUC</italic> criterion in Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3a</xref>). Also, <italic>AUCpr</italic> for PNN30% was always better than PNN15% for all the traits in the upper class. These results lead to the conclusion that PNN was more accurate than MLP for assigning maize lines to the correct upper class for GY under WW and SS conditions. Also under the <italic>AUC</italic> criterion, PNN30% was similar to PNN15% for GY-HI and GY-WW, but better than PNN15% for GY-LO and GY-SS. Under the criterion <italic>AUCpr</italic>, PNN30% was always better than PNN15% for all GY.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Maize datasets</p></caption><table frame="hsides" rules="groups"><tbody><tr><td colspan="9">Upper class</td></tr><tr><td/><td colspan="2">MLP15%</td><td colspan="2">PNN15%</td><td colspan="2">MLP30%</td><td colspan="2">PNN30%</td></tr><tr><td>GY-HI</td><td>0.235</td><td>(0.126)</td><td><bold>0.306</bold></td><td>(0.118)</td><td>0.429</td><td>(0.108)</td><td><bold>0.509</bold></td><td>(0.102)</td></tr><tr><td>GY-LO</td><td>0.168</td><td>(0.065)</td><td><bold>0.188</bold></td><td>(0.076)</td><td>0.358</td><td>(0.107)</td><td><bold>0.408</bold></td><td>(0.107)</td></tr><tr><td>GY-SS</td><td>0.199</td><td>(0.093)</td><td><bold>0.204</bold></td><td>(0.110)</td><td>0.363</td><td>(0.111)</td><td><bold>0.453</bold></td><td>(0.119)</td></tr><tr><td>GY-WW</td><td>0.239</td><td>(0.131)</td><td><bold>0.382</bold></td><td>(0.175)</td><td>0.410</td><td>(0.117)</td><td><bold>0.477</bold></td><td>(0.111)</td></tr><tr><td colspan="9">Middle class</td></tr><tr><td/><td colspan="2">MLP40%</td><td colspan="2">PNN40%</td><td colspan="2">MLP70%</td><td colspan="2">PNN70%</td></tr><tr><td>ASI-SS</td><td>0.465</td><td>(0.096)</td><td><bold>0.495</bold></td><td>(0.092)</td><td>0.724</td><td>(0.076)</td><td><bold>0.746</bold></td><td>(0.074)</td></tr><tr><td>ASI-WW</td><td>0.436</td><td>(0.091)</td><td><bold>0.481</bold></td><td>(0.088)</td><td>0.706</td><td>(0.072)</td><td><bold>0.722</bold></td><td>(0.084)</td></tr><tr><td colspan="9">Lower class</td></tr><tr><td/><td colspan="2">MLP15%</td><td colspan="2">PNN15%</td><td colspan="2">MLP30%</td><td colspan="2">PNN30%</td></tr><tr><td>FFL-SS</td><td>0.185</td><td>(0.087)</td><td><bold>0.288</bold></td><td>(0.137)</td><td>0.383</td><td>(0.106)</td><td><bold>0.465</bold></td><td>(0.096)</td></tr><tr><td>MFL-SS</td><td>0.205</td><td>(0.101)</td><td><bold>0.343</bold></td><td>(0.149)</td><td>0.421</td><td>(0.119)</td><td><bold>0.499</bold></td><td>(0.112)</td></tr><tr><td>FFL-WW</td><td>0.197</td><td>(0.102)</td><td><bold>0.298</bold></td><td>(0.161)</td><td>0.413</td><td>(0.120)</td><td><bold>0.506</bold></td><td>(0.133)</td></tr><tr><td>MFL-WW</td><td>0.199</td><td>(0.094)</td><td><bold>0.288</bold></td><td>(0.155)</td><td>0.437</td><td>(0.133)</td><td><bold>0.516</bold></td><td>(0.139)</td></tr><tr><td>GLS-1</td><td>0.269</td><td>(0.096)</td><td><bold>0.338</bold></td><td>(0.135)</td><td>0.476</td><td>(0.096)</td><td><bold>0.526</bold></td><td>(0.092)</td></tr><tr><td>GLS-2</td><td>0.320</td><td>(0.140)</td><td><bold>0.447</bold></td><td>(0.157)</td><td>0.524</td><td>(0.101)</td><td><bold>0.642</bold></td><td>(0.093)</td></tr><tr><td>GLS-3</td><td>0.372</td><td>(0.138)</td><td><bold>0.456</bold></td><td>(0.149)</td><td>0.496</td><td>(0.128)</td><td><bold>0.589</bold></td><td>(0.116)</td></tr><tr><td>GLS-4</td><td>0.350</td><td>(0.135)</td><td><bold>0.487</bold></td><td>(0.147)</td><td>0.439</td><td>(0.110)</td><td><bold>0.512</bold></td><td>(0.111)</td></tr><tr><td>GLS-5</td><td>0.161</td><td>(0.072)</td><td><bold>0.208</bold></td><td>(0.107)</td><td>0.429</td><td>(0.098)</td><td><bold>0.538</bold></td><td>(0.118)</td></tr><tr><td>GLS-6</td><td>0.320</td><td>(0.091)</td><td><bold>0.400</bold></td><td>(0.109)</td><td>0.431</td><td>(0.094)</td><td><bold>0.491</bold></td><td>(0.098)</td></tr></tbody></table><table-wrap-foot><p>Mean values of the area under the precision-recall curve <italic>AUCpr AUCpr</italic> (standard deviation in parentheses) of 50 random partitions for 15 and 30&#xA0;% upper classes for grain yield (GY) in four environments (HI, LO, SS, and WW), for 40 and 70&#xA0;% middle class for anthesis-silking interval (ASI) in two environments (SS and WW), and for 15 and 30&#xA0;% lower classes for four traits, female flowering (FFL) and male flowering (MFL) in two environments (SS and WW); for gray leaf spot resistance (GLS) in six environments (1&#x2013;6) and for classifiers MLP and PNN. Numbers in bold are the highest <italic>AUCpr</italic> values between MLP and PNN for 15 and 30&#xA0;%</p></table-wrap-foot></table-wrap></p>
        </sec>
        <sec id="Sec15">
          <title>Middle classes (40 and 70&#xA0;%)</title>
          <p>Concerning the <italic>AUC</italic> criterion for the middle class based on ASI-SS and ASI-WW, Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3b</xref> shows a slight superiority of PNN over MLP for both 40 and 70&#xA0;%; however, PNN40% was, on average, slightly better than PNN70%. On the other hand, results using the <italic>AUCpr</italic> criterion also show a slight superiority of PNN over MLP for MLP40% for ASI-SS and MLP70% for both ASI-SS and ASI-WW (Table&#xA0;<xref rid="Tab5" ref-type="table">5</xref>). For this middle class, the <italic>AUCpr</italic> results favored PNN as a better predictor than MLP for assigning maize lines to the correct middle class.</p>
        </sec>
        <sec id="Sec16">
          <title>Lower classes (15 and 30&#xA0;%)</title>
          <p>For the lower class, Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3c</xref> for FL and Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3d</xref> for GLS (both traits in different environments) show a clear superiority in terms of the <italic>AUC</italic> criterion of PNN over MLP for both lower classes. The better prediction accuracy of classifier PNN is reflected in <italic>AUCpr</italic> prediction accuracy, where PNN outperformed MLP for both lower classes, and PNN30% was higher than PNN15% for all 10 traits (Table&#xA0;<xref rid="Tab5" ref-type="table">5</xref>).</p>
        </sec>
      </sec>
      <sec id="Sec17">
        <title>Comparing classifiers for selecting individuals in the upper and lower classes in the wheat datasets</title>
        <sec id="Sec18">
          <title>Upper classes (15 and 30&#xA0;%)</title>
          <p>Results of <italic>AUC</italic> for GY that were selected in the upper 15 and 30&#xA0;% classes are presented in Fig.&#xA0;<xref rid="Fig4" ref-type="fig">4a</xref> and in Additional file <xref rid="MOESM2" ref-type="media">2</xref>: Table S2. PNN outperformed MLP for both upper classes for all GY. PNN30% gave better prediction accuracy than PNN15% in most traits, with the exception of GY-3 and GY-6, where PNN15% had better prediction than PNN30%.</p>
          <p>Criterion <italic>AUCpr</italic> showed that PNN was better than MLP for both upper classes; PNN appeared as the best class predictive models in all GY traits. Furthermore, under the <italic>AUCpr</italic> criterion, PNN30% was higher than PNN15% in all wheat GY traits (Table&#xA0;<xref rid="Tab6" ref-type="table">6</xref>). In summary, results of the upper 15 and 30&#xA0;% classes show that PNN was a more accurate predictor than MLP when using the <italic>AUC</italic> and <italic>AUCpr</italic> criteria.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Wheat datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2">MLP15%</th><th colspan="2">PNN15%</th><th colspan="2">MLP30%</th><th colspan="2">PNN30%</th></tr></thead><tbody><tr><td colspan="9">Upper class</td></tr><tr><td>GY-1</td><td>0.204</td><td>(0.084)</td><td><bold>0.288</bold></td><td>(0.140)</td><td>0.406</td><td>(0.113)</td><td><bold>0.475</bold></td><td>(0.102)</td></tr><tr><td>GY-2</td><td>0.270</td><td>(0.108)</td><td><bold>0.307</bold></td><td>(0.111)</td><td>0.485</td><td>(0.113)</td><td><bold>0.567</bold></td><td>(0.116)</td></tr><tr><td>GY-3</td><td>0.227</td><td>(0.114)</td><td><bold>0.268</bold></td><td>(0.108)</td><td>0.366</td><td>(0.100)</td><td><bold>0.453</bold></td><td>(0.118)</td></tr><tr><td>GY-4</td><td>0.242</td><td>(0.110)</td><td><bold>0.325</bold></td><td>(0.118)</td><td>0.409</td><td>(0.107)</td><td><bold>0.518</bold></td><td>(0.115)</td></tr><tr><td>GY-5</td><td>0.284</td><td>(0.115)</td><td><bold>0.326</bold></td><td>(0.142)</td><td>0.505</td><td>(0.116)</td><td><bold>0.550</bold></td><td>(0.107)</td></tr><tr><td>GY-6</td><td>0.504</td><td>(0.172)</td><td><bold>0.561</bold></td><td>(0.157)</td><td>0.637</td><td>(0.115)</td><td><bold>0.701</bold></td><td>(0.083)</td></tr><tr><td>GY-7</td><td>0.199</td><td>(0.091)</td><td><bold>0.290</bold></td><td>(0.117)</td><td>0.423</td><td>(0.114)</td><td><bold>0.529</bold></td><td>(0.115)</td></tr><tr><td colspan="9">Lower class</td></tr><tr><td>DTH-1</td><td>0.304</td><td>(0.113)</td><td><bold>0.414</bold></td><td>(0.124)</td><td>0.522</td><td>(0.107)</td><td><bold>0.630</bold></td><td>(0.091)</td></tr><tr><td>DTH-2</td><td>0.297</td><td>(0.117)</td><td><bold>0.429</bold></td><td>(0.132)</td><td>0.433</td><td>(0.110)</td><td><bold>0.521</bold></td><td>(0.104)</td></tr><tr><td>DTH-3</td><td>0.364</td><td>(0.149)</td><td><bold>0.511</bold></td><td>(0.151)</td><td>0.547</td><td>(0.115)</td><td><bold>0.650</bold></td><td>(0.095)</td></tr><tr><td>DTH-4</td><td>0.254</td><td>(0.077)</td><td><bold>0.298</bold></td><td>(0.089)</td><td>0.297</td><td>(0.070)</td><td><bold>0.363</bold></td><td>(0.097)</td></tr><tr><td>DTH-5</td><td>0.275</td><td>(0.131)</td><td><bold>0.384</bold></td><td>(0.164)</td><td>0.440</td><td>(0.104)</td><td><bold>0.546</bold></td><td>(0.087)</td></tr><tr><td>DTH-6</td><td>0.380</td><td>(0.091)</td><td><bold>0.467</bold></td><td>(0.094)</td><td>0.465</td><td>(0.099)</td><td><bold>0.520</bold></td><td>(0.112)</td></tr><tr><td>DTH-7</td><td>0.368</td><td>(0.114)</td><td><bold>0.482</bold></td><td>(0.113)</td><td>0.521</td><td>(0.124)</td><td><bold>0.591</bold></td><td>(0.115)</td></tr><tr><td>DTH-8</td><td>0.264</td><td>(0.097)</td><td><bold>0.382</bold></td><td>(0.103)</td><td>0.452</td><td>(0.102)</td><td><bold>0.599</bold></td><td>(0.095)</td></tr><tr><td>DTH-9</td><td>0.261</td><td>(0.103)</td><td><bold>0.367</bold></td><td>(0.112)</td><td>0.416</td><td>(0.099)</td><td><bold>0.535</bold></td><td>(0.107)</td></tr><tr><td>DTH-10</td><td>0.447</td><td>(0.109)</td><td><bold>0.553</bold></td><td>(0.114)</td><td>0.462</td><td>(0.112)</td><td><bold>0.578</bold></td><td>(0.124)</td></tr></tbody></table><table-wrap-foot><p>Mean values of the area under the precision-recall curve <italic>AUCpr</italic> (standard deviation in parentheses) of 50 random partitions for the 15 and 30&#xA0;% upper classes for grain yield (GY) in 7 environments (1&#x2013;7) and 15 and 30&#xA0;% lower classes for days to heading (DTH) in 10 environments (1&#x2013;10) for classifiers MLP and PNN. Numbers in bold are the highest <italic>AUCpr</italic> values between MLP and PNN for 15 and 30&#xA0;%</p></table-wrap-foot></table-wrap></p>
        </sec>
        <sec id="Sec19">
          <title>Lower classes (15 and 30&#xA0;%)</title>
          <p>For the lower classes involving wheat DTH, <italic>AUC</italic> of PNN was higher than MLP for both 15 and 30&#xA0;% percentiles and all traits (Fig.&#xA0;<xref rid="Fig4" ref-type="fig">4b</xref>). In five instances (DTH-2, DTH-3, DTH-5, DTH-6 and DTH-9), the PNN15% model was slightly more accurate than PNN30% when classifying individuals in this lower class.</p>
          <p>The best performance of PNN was reflected in the prediction accuracy given by the <italic>AUCpr</italic> criterion, where PNN was better than MLP in both lower classes for all DTH traits. Likewise, PNN30% was always higher than PNN15% (Table&#xA0;<xref rid="Tab6" ref-type="table">6</xref>).</p>
        </sec>
      </sec>
      <sec id="Sec20">
        <title>Prediction accuracy of PNN classifier with two and three classes in the wheat datasets</title>
        <p>This section compares the performance of PNN in the upper and lower (15 and 30&#xA0;%) classes for wheat GY and DTH traits, when two and three classes are formed and evaluated using the <italic>AUC</italic> (Table&#xA0;<xref rid="Tab7" ref-type="table">7</xref>) and <italic>AUCpr</italic> (Table&#xA0;<xref rid="Tab8" ref-type="table">8</xref>) criteria. For the <italic>AUC</italic> criterion, PNN with three classes was slightly better than PNN with two classes for most traits in the upper and lower 15 and 30&#xA0;% classes (Table&#xA0;<xref rid="Tab7" ref-type="table">7</xref>). For the <italic>AUCpr</italic> criterion, results were not as clear as for <italic>AUC</italic>; however, PNN with three classes was globally better than PNN with two classes (Table&#xA0;<xref rid="Tab8" ref-type="table">8</xref>).<table-wrap id="Tab7"><label>Table 7</label><caption><p>Wheat datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2">PNN15% (two classes)</th><th colspan="2">PNN15% (three classes)</th><th colspan="2">PNN30% (two classes)</th><th colspan="2">PNN30% (three classes)</th></tr></thead><tbody><tr><td colspan="9">Upper class</td></tr><tr><td>GY-1</td><td>0.658</td><td>(0.140)</td><td><bold>0.675</bold></td><td>(0.135)</td><td>0.708</td><td>(0.082)</td><td><bold>0.735</bold></td><td>(0.085)</td></tr><tr><td>GY-2</td><td>0.691</td><td>(0.091)</td><td><bold>0.713</bold></td><td>(0.100)</td><td>0.765</td><td>(0.081)</td><td><bold>0.805</bold></td><td>(0.076)</td></tr><tr><td>GY-3</td><td>0.694</td><td>(0.123)</td><td><bold>0.697</bold></td><td>(0.120)</td><td><bold>0.664</bold></td><td>(0.115)</td><td>0.663</td><td>(0.115)</td></tr><tr><td>GY-4</td><td>0.674</td><td>(0.120)</td><td><bold>0.693</bold></td><td>(0.105)</td><td>0.701</td><td>(0.112)</td><td><bold>0.748</bold></td><td>(0.107)</td></tr><tr><td>GY-5</td><td>0.710</td><td>(0.123)</td><td><bold>0.727</bold></td><td>(0.115)</td><td>0.775</td><td>(0.083)</td><td><bold>0.782</bold></td><td>(0.089)</td></tr><tr><td>GY-6</td><td><bold>0.880</bold></td><td>(0.097)</td><td>0.878</td><td>(0.100)</td><td>0.830</td><td>(0.075)</td><td><bold>0.864</bold></td><td>(0.070)</td></tr><tr><td>GY-7</td><td>0.649</td><td>(0.160)</td><td><bold>0.690</bold></td><td>(0.158)</td><td>0.708</td><td>(0.116)</td><td><bold>0.736</bold></td><td>(0.106)</td></tr><tr><td colspan="9">Lower class</td></tr><tr><td>DTH-1</td><td>0.724</td><td>(0.112)</td><td><bold>0.779</bold></td><td>(0.109)</td><td><bold>0.791</bold></td><td>(0.074)</td><td><bold>0.791</bold></td><td>(0.072)</td></tr><tr><td>DTH-2</td><td>0.773</td><td>(0.094)</td><td><bold>0.789</bold></td><td>(0.090)</td><td><bold>0.763</bold></td><td>(0.100)</td><td>0.751</td><td>(0.092)</td></tr><tr><td>DTH-3</td><td>0.840</td><td>(0.100)</td><td><bold>0.843</bold></td><td>(0.101)</td><td>0.802</td><td>(0.074)</td><td><bold>0.806</bold></td><td>(0.074)</td></tr><tr><td>DTH-4</td><td>0.584</td><td>(0.098)</td><td><bold>0.585</bold></td><td>(0.097)</td><td>0.568</td><td>(0.094)</td><td><bold>0.587</bold></td><td>(0.102)</td></tr><tr><td>DTH-5</td><td>0.763</td><td>(0.121)</td><td><bold>0.779</bold></td><td>(0.128)</td><td>0.754</td><td>(0.075)</td><td><bold>0.756</bold></td><td>(0.072)</td></tr><tr><td>DTH-6</td><td>0.708</td><td>(0.086)</td><td><bold>0.736</bold></td><td>(0.085)</td><td><bold>0.722</bold></td><td>(0.097)</td><td><bold>0.722</bold></td><td>(0.098)</td></tr><tr><td>DTH-7</td><td>0.765</td><td>(0.096)</td><td><bold>0.775</bold></td><td>(0.095)</td><td>0.775</td><td>(0.097)</td><td><bold>0.785</bold></td><td>(0.088)</td></tr><tr><td>DTH-8</td><td>0.750</td><td>(0.080)</td><td><bold>0.755</bold></td><td>(0.082)</td><td><bold>0.803</bold></td><td>(0.065)</td><td>0.799</td><td>(0.067)</td></tr><tr><td>DTH-9</td><td>0.764</td><td>(0.105)</td><td><bold>0.774</bold></td><td>(0.090)</td><td>0.736</td><td>(0.088)</td><td><bold>0.743</bold></td><td>(0.087)</td></tr><tr><td>DTH-10</td><td>0.763</td><td>(0.763)</td><td><bold>0.768</bold></td><td>(0.098)</td><td>0.774</td><td>(0.094)</td><td><bold>0.787</bold></td><td>(0.102)</td></tr></tbody></table><table-wrap-foot><p>Mean values of the area under the ROC curve <italic>AUC</italic> (standard deviation in parentheses) of 50 random partitions for the 15 and 30&#xA0;% upper class for grain yield (GY) in 7 environments (1&#x2013;7) and for 15 and 30&#xA0;% lower class for days to heading (DTH) for classifier PNN with two and three classes. Numbers in bold are the highest AUC values</p></table-wrap-foot></table-wrap><table-wrap id="Tab8"><label>Table 8</label><caption><p>Wheat datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2">PNN15% (two classes)</th><th colspan="2">PNN15% (three classes)</th><th colspan="2">PNN30% (two classes)</th><th colspan="2">PNN30% (three classes)</th></tr></thead><tbody><tr><td colspan="9">Upper class</td></tr><tr><td>GY-1</td><td>0.270</td><td>(0.134)</td><td><bold>0.288</bold></td><td>(0.140)</td><td><bold>0.499</bold></td><td>(0.118)</td><td>0.475</td><td>(0.102)</td></tr><tr><td>GY-2</td><td><bold>0.322</bold></td><td>(0.118)</td><td>0.307</td><td>(0.111)</td><td>0.538</td><td>(0.117)</td><td><bold>0.567</bold></td><td>(0.116)</td></tr><tr><td>GY-3</td><td><bold>0.310</bold></td><td>(0.138)</td><td>0.268</td><td>(0.108)</td><td>0.452</td><td>(0.117)</td><td><bold>0.453</bold></td><td>(0.118)</td></tr><tr><td>GY-4</td><td>0.319</td><td>(0.121)</td><td><bold>0.325</bold></td><td>(0.118)</td><td>0.482</td><td>(0.112)</td><td><bold>0.518</bold></td><td>(0.115)</td></tr><tr><td>GY-5</td><td><bold>0.333</bold></td><td>(0.161)</td><td>0.326</td><td>(0.142)</td><td>0.545</td><td>(0.104)</td><td><bold>0.550</bold></td><td>(0.107)</td></tr><tr><td>GY-6</td><td><bold>0.562</bold></td><td>(0.159)</td><td>0.561</td><td>(0.157)</td><td>0.668</td><td>(0.087)</td><td><bold>0.701</bold></td><td>(0.083)</td></tr><tr><td>GY-7</td><td>0.263</td><td>(0.124)</td><td><bold>0.290</bold></td><td>(0.117)</td><td>0.503</td><td>(0.117)</td><td><bold>0.529</bold></td><td>(0.115)</td></tr><tr><td colspan="9">Lower class</td></tr><tr><td>DTH-1</td><td>0.370</td><td>(0.114)</td><td><bold>0.414</bold></td><td>(0.124)</td><td>0.629</td><td>(0.091)</td><td><bold>0.630</bold></td><td>(0.091)</td></tr><tr><td>DTH-2</td><td>0.417</td><td>(0.134)</td><td><bold>0.429</bold></td><td>(0.132)</td><td><bold>0.548</bold></td><td>(0.116)</td><td>0.521</td><td>(0.104)</td></tr><tr><td>DTH-3</td><td>0.506</td><td>(0.158)</td><td><bold>0.511</bold></td><td>(0.151)</td><td>0.641</td><td>(0.093)</td><td><bold>0.650</bold></td><td>(0.095)</td></tr><tr><td>DTH-4</td><td>0.292</td><td>(0.090)</td><td><bold>0.298</bold></td><td>(0.089)</td><td>0.350</td><td>(0.094)</td><td><bold>0.363</bold></td><td>(0.097)</td></tr><tr><td>DTH-5</td><td>0.355</td><td>(0.158)</td><td><bold>0.384</bold></td><td>(0.164)</td><td><bold>0.551</bold></td><td>(0.091)</td><td>0.546</td><td>(0.087)</td></tr><tr><td>DTH-6</td><td>0.444</td><td>(0.087)</td><td><bold>0.467</bold></td><td>(0.094)</td><td><bold>0.530</bold></td><td>(0.119)</td><td>0.520</td><td>(0.112)</td></tr><tr><td>DTH-7</td><td>0.462</td><td>(0.116)</td><td><bold>0.482</bold></td><td>(0.113)</td><td>0.580</td><td>(0.122)</td><td><bold>0.591</bold></td><td>(0.115)</td></tr><tr><td>DTH-8</td><td><bold>0.387</bold></td><td>(0.104)</td><td>0.382</td><td>(0.103)</td><td><bold>0.603</bold></td><td>(0.089)</td><td>0.599</td><td>(0.095)</td></tr><tr><td>DTH-9</td><td><bold>0.376</bold></td><td>(0.138)</td><td>0.367</td><td>(0.112)</td><td>0.532</td><td>(0.105)</td><td><bold>0.535</bold></td><td>(0.107)</td></tr><tr><td>DTH-10</td><td><bold>0.557</bold></td><td>(0.112)</td><td>0.553</td><td>(0.114)</td><td>0.575</td><td>(0.117)</td><td><bold>0.578</bold></td><td>(0.124)</td></tr></tbody></table><table-wrap-foot><p>Mean values of the area under the precision-recall curve <italic>AUCpr</italic> (standard deviation in parentheses) of 50 random partitions for the 15 and 30&#xA0;% upper classes for grain yield (GY) in 7 environments (1&#x2013;7) and for 15 and 30&#xA0;% lower classes for days to heading (DTH) for classifier PNN with two and three classes. Numbers in bold are the highest AUCpr values</p></table-wrap-foot></table-wrap></p>
        <p>In summary, results for the wheat datasets comparing the performance of PNN for selecting individuals in the lower and upper 15 and 30&#xA0;% classes, based on the splitting of continuous traits into two or three classes, showed that for the lower 15&#xA0;%, the performance of PNN with three classes was better than PNN with two classes (in seven of ten traits). However, PNN with two classes gave better predictions than PNN with three classes in the upper 15&#xA0;% (four over seven traits). This is not the case when predicting individuals in the upper and lower 30&#xA0;%, where PNN with three classes was a better predictor than PNN with two classes for most traits.</p>
      </sec>
      <sec id="Sec21">
        <title>ROC and precision-recall curves for the maize and wheat datasets</title>
        <p>Some results of the ROC and precision-recall curves for various maize and wheat datasets for upper and lower 15 and 30&#xA0;%, with the middle class in maize for 40 and 70&#xA0;%, are displayed in a series of figures (for the maize datasets, Fig.&#xA0;<xref rid="Fig5" ref-type="fig">5a</xref>&#x2013;<xref rid="Fig5" ref-type="fig">f</xref>; for the wheat datasets, Fig.&#xA0;<xref rid="Fig6" ref-type="fig">6a</xref>&#x2013;<xref rid="Fig6" ref-type="fig">d</xref>). For the maize and wheat datasets, it is clear that the ROC curves of PNN for the upper and lower 15 and 30&#xA0;% and the middle 40 and 70&#xA0;% dominated the corresponding curves of MLP. Also, <italic>AUC</italic> values for PNN were always greater than those for MLP.<fig id="Fig5"><label>Fig. 5</label><caption><p>The upper curve is the ROC curve (<italic>AUC</italic>) with recall vs false positive rate. The lower curve is the precision-recall curve <italic>AUCpr</italic> with precision vs recall for the <bold>a</bold> upper 15&#xA0;% class of grain yield under well-watered conditions (GY-WW) of classifiers MLP (green) and PNN (blue); <bold>b</bold> upper 30&#xA0;% class of trait grain yield under well-watered conditions (GY-WW) of MLP (green) and PNN (blue); <bold>c</bold> middle 40&#xA0;% class of trait anthesis-silking interval under well-watered conditions (ASI-WW) of MLP (green) and PNN (blue) and <bold>d</bold> middle 70&#xA0;% class of trait anthesis-silking interval under well-watered conditions (ASI-WW) of MLP (green) and PNN (blue); <bold>e</bold> lower 15&#xA0;% class of trait female flowering under well-watered conditions (FFL-WW) of MLP (green) and PNN (blue); <bold>f</bold> lower 30&#xA0;% class of trait female flowering under well-watered conditions (FFL-WW) of MLP (green) and PNN (blue)</p></caption><graphic xlink:href="12864_2016_2553_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>The upper curve is the ROC curve (<italic>AUC</italic>) with recall (sensitivity) vs false positive rate. The lower curve is the precision-recall curve <italic>AUCpr</italic> with precision vs recall for the <bold>a</bold> upper 15&#xA0;% class of grain yield in environment 6 (GY-6) of classifiers MLP (green) and PNN (blue); <bold>b</bold> upper 30&#xA0;% class of grain yield in environment 6 (GY-6) of MLP (green) and PNN (blue); <bold>c</bold> lower 15&#xA0;% class of days to heading in environment 3 (DTH-3) of MLP (green) and PNN (blue); <bold>d</bold> lower 30&#xA0;% class of days to heading in environment 3 (DTH-3) of MLP (green) and PNN (blue)</p></caption><graphic xlink:href="12864_2016_2553_Fig6_HTML" id="MO6"/></fig></p>
        <p>Furthermore, the <italic>P</italic> vs <italic>R</italic> graphs show that for all the maize and wheat datasets, PNN was better than MLP, indicating that the precision of PNN remains better than that of the MLP for all recall values. The precision of PNN started declining at higher values of R than the values of R for MLP.</p>
      </sec>
    </sec>
    <sec id="Sec22" sec-type="discussion">
      <title>Discussion</title>
      <sec id="Sec23">
        <title>Accuracy of the MLP and PNN classifiers for selecting the best individuals</title>
        <p>Genomic selection aims to accurately predict genetic values with genome-wide marker data using a three-step process: 1) model training and validation, 2) predicting genetic values, and 3) selecting based on these predictions [<xref ref-type="bibr" rid="CR34">34</xref>].</p>
        <p>We evaluated the performance of classifiers MLP and PNN for selecting the best individuals in maize and wheat datasets (Tables&#xA0;<xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>). Results indicated that, overall, PNN was more precise in identifying individuals in the correct class than MLP. Previous studies using RBFNN and Bayesian regularized NN on the same wheat datasets [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR11">11</xref>] used in this study showed their prediction advantage over the linear parametric models for complex traits such as GY because these models can capture cryptic epistatic effects in gene&#x2009;&#xD7;&#x2009;gene networks such as those usually present in wheat (e.g., additive&#x2009;&#xD7;&#x2009;additive interactions). The good performance of PNN for selecting individuals in the correct classes may also be due to its ability for capturing small and complex interactions, while MLP may fail to do so.</p>
        <p>The fact that these classifiers are trained to maximize the probability of membership of an individual to the target class, rather than searching for an overall performance, makes it attractive for applying these tools in GS. Results from MLP and PNN indicated that PNN was much more efficient in maximizing the probability of membership for the upper, middle, and lower classes than MLP.</p>
        <p>From a practical genome-assisted plant breeding perspective, this study attempts to mimic the breeder&#x2019;s decision, for example when selecting the upper 15 or 30&#xA0;% class candidates for GY, or when selecting the lower 15 or 30&#xA0;% class candidates for DTH, GLS or FL. In maize breeding, ASI synchrony close to zero is a crucial &#x201C;middle class trait&#x201D; under SS conditions because it will ensure selecting plants that will simultaneously produce pollen and silk; thus grains can be harvested. Therefore, PNN should help genomic-assisted breeding select appropriate candidates in each class of interest.</p>
        <p>Breeding values have two main components, parental average (accounting for between family variation) and Mendelian sampling (accounting for within family variation). Genomic prediction should account for these two main components and try to control potential population structures that could modify prediction accuracy between the selected training and testing populations. An important practical question is how well PNN and MLP predict the breeding value of individuals between families and within families that were not phenotyped. Although the elite maize and wheat lines used in this study are not ideal as training sets, the cross-validation scheme used in this study (where 50 random partitions stratified by classes were generated for each data set) attempts to mimic the prediction of non phenotyped individuals belonging to different families (crosses) or to the same family. Although this cross-validation design may not have chosen individuals between and within families as precisely as they are in reality, it is likely that the 50 random partitions searched for all possible relationships between individuals in the training and testing sets such that some cross-validation partitions selected subsets of training data that had high correlations with the observed data, indicating a family relationship among individuals belonging to those training&#x2013;testing subsets [<xref ref-type="bibr" rid="CR11">11</xref>], whereas other random partitions chose subsets of training individuals that had no family relationship with those in the testing set, thus producing low correlations with the observed values. When applied to both classifiers, PNN consistently gave better average prediction accuracy (across the 50 random partitions) of the genetic values of the unobserved individuals than MLP in all 33 maize and wheat data sets.</p>
      </sec>
      <sec id="Sec24">
        <title><italic>AUC</italic> and <italic>AUCpr</italic></title>
        <p>For both datasets, the results of the <italic>AUCpr</italic> criterion showed that the values of the upper and lower PNN30% were higher than those for the upper and lower PNN15%. Also, the values of the middle PNN70% were higher than those for the PNN40% (Tables&#xA0;<xref rid="Tab5" ref-type="table">5</xref> and <xref rid="Tab6" ref-type="table">6</xref>). These results were similar but not equal to those found by <italic>AUC</italic> (which does not account for imbalances in the number of individuals comprising the upper, middle and lower classes) in several instances. PNN15% was superior to PNN30% in the maize data (e.g., ASI-SS, ASI-WW, FFL-SS, MFL-SS, GL-1, GLS-4) and the wheat data (e.g., DTH-2, DTH-3, DTH5, DTH-6, DTH-9). Prediction accuracy of individuals was clearly hampered under biotic stress in the maize data, which was also found by [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR35">35</xref>].</p>
        <p>Figures&#xA0;<xref rid="Fig5" ref-type="fig">5a</xref>&#x2013;<xref rid="Fig5" ref-type="fig">f</xref> and <xref rid="Fig6" ref-type="fig">6a</xref>&#x2013;<xref rid="Fig6" ref-type="fig">d</xref> showing the ROC curve clearly indicated the advantage of PNN over MLP. The <italic>R</italic> vs <italic>fpr</italic> graph indicates that, for most of the traits, the probability of correctly classifying an individual in the upper, lower or middle classes was very often 0.80 or higher, even with a small <italic>fpr</italic>. In most cases, at a value of <italic>fpr</italic>&#x2009;=&#x2009;0, the probability of classifying an individual in the correct class was 0.80 or greater for PNN. For all traits, the <italic>AUC</italic> of PNN15% was always better than the <italic>AUC</italic> of MLP15% and the <italic>AUC</italic> of PNN30% was better than the <italic>AUC</italic> of MLP30%.</p>
        <p>For the <italic>AUCpr</italic> curve, Figs.&#xA0;<xref rid="Fig5" ref-type="fig">5a</xref>&#x2013;<xref rid="Fig5" ref-type="fig">f</xref> and <xref rid="Tab6" ref-type="table">6a</xref>&#x2013;<xref rid="Tab6" ref-type="table">d</xref> indicate that, in most cases, PNN had higher precision than MLP at higher sensitivity values. This criterion also indicates the superior performance of PNN over MLP.</p>
      </sec>
      <sec id="Sec25">
        <title>Prediction accuracy for 30 vs 15&#xA0;% classes with binary and trichotomous classes</title>
        <p>Based on the <italic>AUC</italic> criterion, it is clear that PNN gave better prediction accuracy than MLP when assigning maize and wheat individuals to the classes of interest. Using the <italic>AUCpr</italic> criterion, the results were equally clear for the wheat and the maize datasets.</p>
        <p>For the wheat datasets, the <italic>AUC</italic> criterion showed the superiority of PNN30% with three classes over PNN30% with two classes, as well as the superiority of PNN15% with three classes over PNN15% with two classes (Table&#xA0;<xref rid="Tab7" ref-type="table">7</xref>). However, the differences given by the <italic>AUC</italic> criterion were not as marked as those shown by the <italic>AUCpr</italic> criterion. The <italic>AUCpr</italic> criterion applied with PNN shows that for the upper 15&#xA0;% classes (GY traits), partitioning the data into two classes assigned more wheat lines to the correct observed classes than partitioning the data into three classes. However, for the lower 15&#xA0;% classes (DTH traits) and for PNN 30&#xA0;% upper and lower classes, results indicate that three classes gave better prediction than two classes (Table&#xA0;<xref rid="Tab8" ref-type="table">8</xref>).</p>
      </sec>
    </sec>
    <sec id="Sec26" sec-type="conclusion">
      <title>Conclusions</title>
      <p>We compared the performance of the multilayer perceptron (MLP) and the probabilistic neural network (PNN) classifiers for selecting the best individuals belonging to a class of interest (target class) in maize and wheat datasets using high-throughput molecular marker information (55&#xA0;k and 1.4&#xA0;k). PNN outperformed MLP in most of the datasets. The performance criteria used to judge the predictive accuracy of MLP and PNN for assigning individuals to the right observed class were the area under ROC curve, <italic>AUC,</italic> and the area under the precision-recall curve, <italic>AUCpr</italic>, PNN had better accuracy than MLP. In genomic selection, where p markers &gt;&#x2009;&gt;&#x2009;n individuals is the norm, PNN seems promising because of its better generalization capacity than MLP, and is faster than MLP in obtaining optimal solutions, thus presenting appealing computational advantages.</p>
      <sec id="Sec27">
        <title>Availability of supporting data</title>
        <p>The 33 datasets (16 maize and 17 wheat trials) and the MATLAB scripts used in this work are available at <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/11529/10576">http://hdl.handle.net/11529/10576</ext-link>.</p>
      </sec>
    </sec>
  </body>
  <back>
    <app-group>
      <app id="App1">
        <sec id="Sec28">
          <title>Additional files</title>
          <p>
            <media position="anchor" xlink:href="12864_2016_2553_MOESM1_ESM.doc" id="MOESM1">
              <label>Additional file 1: Table S1.</label>
              <caption>
                <p>Maize datasets. Mean values of the area under the ROC curve <italic>AUC</italic> (standard deviation in parentheses) of 50 random partitions for upper 15 and 30&#xA0;% classes for grain yield (GY) in four environments (HI, LO, SS, and SS), for middle 40 and 70&#xA0;% classes for anthesis-silking interval (ASI) in two environments (SS) and (WW), and for lower 15 and 30&#xA0;% classes for four traits, female flowering (FFL) and male flowering (MFL) in two environments (WW and SS); for gray leaf spot resistance (GLS) in six environments and for both MLP and PNN classifiers. Numbers in bold are the highest <italic>AUC</italic> values between MLP and PNN for 15 and 30&#xA0;%. (DOC 62&#xA0;kb)</p>
              </caption>
            </media>
            <media position="anchor" xlink:href="12864_2016_2553_MOESM2_ESM.doc" id="MOESM2">
              <label>Additional file 2 :Table S2.</label>
              <caption>
                <p>Wheat datasets. Mean values of the area under the ROC curve <italic>AUC</italic> (standard deviation in parentheses) of 50 random partitions for the upper 15 and 30&#xA0;% classes for grain yield (GY) in seven environments (1-7) and for the lower 15 and 30&#xA0;% classes for days to heading (DTH) in ten environments (1-10) for both MLP and PNN classifiers. Numbers in bold are the highest <italic>AUC</italic> values between MLP and PNN for 15 and 30&#xA0;%. (DOC 59&#xA0;kb)</p>
              </caption>
            </media>
          </p>
        </sec>
      </app>
    </app-group>
    <glossary>
      <title>Abbreviations</title>
      <def-list>
        <def-item>
          <term>ASI</term>
          <def>
            <p>anthesis-silking interval</p>
          </def>
        </def-item>
        <def-item>
          <term>
            <italic>AUC</italic>
          </term>
          <def>
            <p>area under the receiver operating characteristic curve</p>
          </def>
        </def-item>
        <def-item>
          <term>
            <italic>AUCpr</italic>
          </term>
          <def>
            <p>area under the precision-recall curve</p>
          </def>
        </def-item>
        <def-item>
          <term>DTH</term>
          <def>
            <p>days to heading</p>
          </def>
        </def-item>
        <def-item>
          <term>FFL</term>
          <def>
            <p>female flowering or days to silking</p>
          </def>
        </def-item>
        <def-item>
          <term>GLS</term>
          <def>
            <p>gray leaf spot</p>
          </def>
        </def-item>
        <def-item>
          <term>GS</term>
          <def>
            <p>genomic selection</p>
          </def>
        </def-item>
        <def-item>
          <term>GY</term>
          <def>
            <p>grain yield</p>
          </def>
        </def-item>
        <def-item>
          <term>HI</term>
          <def>
            <p>high yielding</p>
          </def>
        </def-item>
        <def-item>
          <term>LO</term>
          <def>
            <p>low yielding</p>
          </def>
        </def-item>
        <def-item>
          <term>MFL</term>
          <def>
            <p>male flowering or days to anthesis</p>
          </def>
        </def-item>
        <def-item>
          <term>MLP</term>
          <def>
            <p>multi-layer perceptron</p>
          </def>
        </def-item>
        <def-item>
          <term>NN</term>
          <def>
            <p>neural networks</p>
          </def>
        </def-item>
        <def-item>
          <term>
            <italic>P</italic>
          </term>
          <def>
            <p>Precision</p>
          </def>
        </def-item>
        <def-item>
          <term>PNN</term>
          <def>
            <p>probabilistic neural network</p>
          </def>
        </def-item>
        <def-item>
          <term>QTL</term>
          <def>
            <p>quantitative trait loci</p>
          </def>
        </def-item>
        <def-item>
          <term>
            <italic>R</italic>
          </term>
          <def>
            <p>recall or sensitivity</p>
          </def>
        </def-item>
        <def-item>
          <term>RBF</term>
          <def>
            <p>radial basis function</p>
          </def>
        </def-item>
        <def-item>
          <term>RBFNN</term>
          <def>
            <p>radial basis function neural network</p>
          </def>
        </def-item>
        <def-item>
          <term>RKHS</term>
          <def>
            <p>reproducing kernel Hilbert spaces</p>
          </def>
        </def-item>
        <def-item>
          <term>ROC</term>
          <def>
            <p>receiver operating characteristic curve</p>
          </def>
        </def-item>
        <def-item>
          <term>SS</term>
          <def>
            <p>severe drought stress</p>
          </def>
        </def-item>
        <def-item>
          <term>WW</term>
          <def>
            <p>well-watered</p>
          </def>
        </def-item>
      </def-list>
    </glossary>
    <fn-group>
      <fn>
        <p>
          <bold>Competing interests</bold>
        </p>
        <p>The authors declare that they have no competing interests.</p>
      </fn>
      <fn>
        <p>
          <bold>Authors&#x2019; contributions</bold>
        </p>
        <p>JMGC conceived, drafted the manuscript, carried out the study, performed computations and wrote a part of the manuscript; JC conceived and wrote part of the manuscript; PPR, LO and DG helped to provide critical insights and revised the manuscript. All authors read and approved the final manuscript.</p>
      </fn>
    </fn-group>
    <ack>
      <p>We thank CIMMYT&#x2019;s global wheat and maize programs and the national program researchers who performed the experiments and collected the valuable phenotypic data analyzed in this study. We acknowledge the financial support provided to Cornell-CIMMYT by the Bill &amp; Melinda Gates Foundation.</p>
    </ack>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>de los Campos</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Hickey</surname>
              <given-names>JM</given-names>
            </name>
            <name>
              <surname>Pong-Wong</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Daetwyler</surname>
              <given-names>HD</given-names>
            </name>
            <name>
              <surname>Calus</surname>
              <given-names>MPL</given-names>
            </name>
          </person-group>
          <article-title>Whole genome regression and prediction methods applied to plant and animal breeding</article-title>
          <source>Genetics</source>
          <year>2013</year>
          <volume>193</volume>
          <issue>2</issue>
          <fpage>327</fpage>
          <lpage>45</lpage>
          <pub-id pub-id-type="doi">10.1534/genetics.112.143313</pub-id>
          <?supplied-pmid 22745228?>
          <pub-id pub-id-type="pmid">22745228</pub-id>
        </element-citation>
      </ref>
      <ref id="CR2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Priors in whole genome regression: the Bayesian alphabet returns</article-title>
          <source>Genetics</source>
          <year>2013</year>
          <volume>194</volume>
          <fpage>573</fpage>
          <lpage>96</lpage>
          <pub-id pub-id-type="doi">10.1534/genetics.113.151753</pub-id>
          <?supplied-pmid 23636739?>
          <pub-id pub-id-type="pmid">23636739</pub-id>
        </element-citation>
      </ref>
      <ref id="CR3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Meuwissen</surname>
              <given-names>THE</given-names>
            </name>
            <name>
              <surname>Hayes</surname>
              <given-names>BJ</given-names>
            </name>
            <name>
              <surname>Goddard</surname>
              <given-names>ME</given-names>
            </name>
          </person-group>
          <article-title>Prediction of total genetic values using genome-wide dense marker maps</article-title>
          <source>Genetics</source>
          <year>2001</year>
          <volume>157</volume>
          <fpage>1819</fpage>
          <lpage>29</lpage>
          <?supplied-pmid 11290733?>
          <pub-id pub-id-type="pmid">11290733</pub-id>
        </element-citation>
      </ref>
      <ref id="CR4">
        <label>4.</label>
        <mixed-citation publication-type="other">de los Campos G, Naya H, Gianola D, Crossa J, Legarra A, Manfredi E, Weigel K, Cotes JM. Predicting quantitative traits with regression models for dense molecular markers and pedigrees. Genetics. 2009;182:375&#x2013;85.</mixed-citation>
      </ref>
      <ref id="CR5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>de los Campos</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Rosa</surname>
              <given-names>GJM</given-names>
            </name>
            <name>
              <surname>Weigel</surname>
              <given-names>KA</given-names>
            </name>
            <name>
              <surname>Crossa</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Semi-parametric genomic-enabled prediction of genetic values using reproducing kernel Hilbert spaces methods</article-title>
          <source>Genet Res</source>
          <year>2010</year>
          <volume>92</volume>
          <issue>4</issue>
          <fpage>295</fpage>
          <lpage>308</lpage>
          <pub-id pub-id-type="doi">10.1017/S0016672310000285</pub-id>
        </element-citation>
      </ref>
      <ref id="CR6">
        <label>6.</label>
        <mixed-citation publication-type="other">Crossa J, de los Campos G, P&#xE9;rez-Rodr&#xED;guez P, Gianola D, Burgue&#xF1;o J, Araus JL, Makumbi D, Dreisigacker S, Yan J, Arief V, Banziger M, Braun, HJ. Prediction of genetic values of quantitative traits in plant breeding using pedigree and molecular markers. Genetics. 2010;186:713&#x2013;24.</mixed-citation>
      </ref>
      <ref id="CR7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Crossa</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>P&#xE9;rez-Rodr&#xED;guez</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>de los Campos</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Mahuku</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Dreisigacker</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Magorokosho</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <article-title>Genomic selection and prediction in plant breeding</article-title>
          <source>J of Crop Improvement</source>
          <year>2011</year>
          <volume>25</volume>
          <issue>3</issue>
          <fpage>239</fpage>
          <lpage>61</lpage>
          <pub-id pub-id-type="doi">10.1080/15427528.2011.558767</pub-id>
        </element-citation>
      </ref>
      <ref id="CR8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gonz&#xE1;lez-Camacho</surname>
              <given-names>JM</given-names>
            </name>
            <name>
              <surname>de los Campos</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>P&#xE9;rez-Rodr&#xED;guez</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Cairns</surname>
              <given-names>JE</given-names>
            </name>
            <name>
              <surname>Mahuku</surname>
              <given-names>G</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Genome-enabled prediction of genetic values using radial basis function neural networks</article-title>
          <source>Theor Appl Genet</source>
          <year>2012</year>
          <volume>125</volume>
          <issue>4</issue>
          <fpage>759</fpage>
          <lpage>71</lpage>
          <pub-id pub-id-type="doi">10.1007/s00122-012-1868-9</pub-id>
          <?supplied-pmid 22566067?>
          <pub-id pub-id-type="pmid">22566067</pub-id>
        </element-citation>
      </ref>
      <ref id="CR9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Heslot</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>HP</given-names>
            </name>
            <name>
              <surname>Sorrells</surname>
              <given-names>ME</given-names>
            </name>
            <name>
              <surname>Jannink</surname>
              <given-names>JL</given-names>
            </name>
          </person-group>
          <article-title>Genomic selection in plant breeding: A comparison of models</article-title>
          <source>Crop Sci</source>
          <year>2012</year>
          <volume>52</volume>
          <fpage>146</fpage>
          <lpage>60</lpage>
          <pub-id pub-id-type="doi">10.2135/cropsci2011.06.0297</pub-id>
        </element-citation>
      </ref>
      <ref id="CR10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>P&#xE9;rez-Rodr&#xED;guez</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>de los Campos</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Crossa</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Genomic-enabled prediction based on molecular markers and pedigree using the BLR package in R</article-title>
          <source>The Plant Genome</source>
          <year>2010</year>
          <volume>3</volume>
          <issue>2</issue>
          <fpage>106</fpage>
          <lpage>16</lpage>
          <pub-id pub-id-type="doi">10.3835/plantgenome2010.04.0005</pub-id>
          <pub-id pub-id-type="pmid">21566722</pub-id>
        </element-citation>
      </ref>
      <ref id="CR11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>P&#xE9;rez-Rodr&#xED;guez</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Gonz&#xE1;lez-Camacho</surname>
              <given-names>JM</given-names>
            </name>
            <name>
              <surname>Crossa</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Man&#xE8;s</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Dreisigacker</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Comparison between linear and non-parametric regression models for genome-enabled prediction in wheat</article-title>
          <source>G3: Genes|Genomes|Genetics</source>
          <year>2012</year>
          <volume>2</volume>
          <issue>12</issue>
          <fpage>1595</fpage>
          <lpage>605</lpage>
          <pub-id pub-id-type="doi">10.1534/g3.112.003665</pub-id>
          <?supplied-pmid 23275882?>
          <pub-id pub-id-type="pmid">23275882</pub-id>
        </element-citation>
      </ref>
      <ref id="CR12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Fernando</surname>
              <given-names>RL</given-names>
            </name>
            <name>
              <surname>Stella</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Genomic-assisted prediction of genetic values with semiparametric procedures</article-title>
          <source>Genetics</source>
          <year>2006</year>
          <volume>173</volume>
          <fpage>1761</fpage>
          <lpage>76</lpage>
          <pub-id pub-id-type="doi">10.1534/genetics.105.049510</pub-id>
          <?supplied-pmid 16648593?>
          <pub-id pub-id-type="pmid">16648593</pub-id>
        </element-citation>
      </ref>
      <ref id="CR13">
        <label>13.</label>
        <mixed-citation publication-type="other">Gianola D, Okut H, Weigel KA, Rosa GJM. Predicting complex quantitative traits with neural networks: a case study with Jersey cows and wheat. BMC Genetics 2011; doi:101186/1471-2156-12-87.</mixed-citation>
      </ref>
      <ref id="CR14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>van Kaam</surname>
              <given-names>JBCHM</given-names>
            </name>
          </person-group>
          <article-title>Reproducing kernel Hilbert space regression methods for genomic-assisted prediction of quantitative traits</article-title>
          <source>Genetics</source>
          <year>2008</year>
          <volume>178</volume>
          <fpage>2289</fpage>
          <lpage>303</lpage>
          <pub-id pub-id-type="doi">10.1534/genetics.107.084285</pub-id>
          <?supplied-pmid 18430950?>
          <pub-id pub-id-type="pmid">18430950</pub-id>
        </element-citation>
      </ref>
      <ref id="CR15">
        <label>15.</label>
        <mixed-citation publication-type="other">Okser S, Pahikkala T, Airola A, Salakoski T, Ripatti S, Aittokallio, T. Regularized machine learning in the genetic prediction of complex traits. PLoS Genetics 2014; doi:10.1371/journal.pgen.1004754.</mixed-citation>
      </ref>
      <ref id="CR16">
        <label>16.</label>
        <mixed-citation publication-type="other">Ehret A, Hochstuhl D, Gianola D, Thaller G. Application of neural networks with back-propagation to genome-enabled prediction of complex traits in Holstein-Friesian and German Fleckvieh cattle. Genetics Selection Evolution 2015; doi:10.1186/s12711-015-0097-5.</mixed-citation>
      </ref>
      <ref id="CR17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sant&#x2019;Anna</surname>
              <given-names>IC</given-names>
            </name>
            <name>
              <surname>Tomaz</surname>
              <given-names>RS</given-names>
            </name>
            <name>
              <surname>Silva</surname>
              <given-names>GN</given-names>
            </name>
            <name>
              <surname>Nascimento</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Bhering</surname>
              <given-names>LL</given-names>
            </name>
            <name>
              <surname>Cruz</surname>
              <given-names>CD</given-names>
            </name>
          </person-group>
          <article-title>Superiority of artificial neural networks for a genetic classification procedure</article-title>
          <source>Genet Mol Res</source>
          <year>2015</year>
          <volume>14</volume>
          <issue>3</issue>
          <fpage>9898</fpage>
          <lpage>906</lpage>
          <pub-id pub-id-type="doi">10.4238/2015.August.19.24</pub-id>
          <?supplied-pmid 26345924?>
          <pub-id pub-id-type="pmid">26345924</pub-id>
        </element-citation>
      </ref>
      <ref id="CR18">
        <label>18.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Hastie</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Tibshirani</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Friedman</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <source>The elements of statistical learning: data mining, inference, and prediction</source>
          <year>2009</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Springer</publisher-name>
        </element-citation>
      </ref>
      <ref id="CR19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Libbrecht</surname>
              <given-names>MW</given-names>
            </name>
            <name>
              <surname>Noble</surname>
              <given-names>WS</given-names>
            </name>
          </person-group>
          <article-title>Machine learning applications in genetics and genomics</article-title>
          <source>Nat Rev Genet</source>
          <year>2015</year>
          <volume>16</volume>
          <issue>6</issue>
          <fpage>321</fpage>
          <lpage>32</lpage>
          <pub-id pub-id-type="doi">10.1038/nrg3920</pub-id>
          <?supplied-pmid 25948244?>
          <pub-id pub-id-type="pmid">25948244</pub-id>
        </element-citation>
      </ref>
      <ref id="CR20">
        <label>20.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kecman</surname>
              <given-names>V</given-names>
            </name>
          </person-group>
          <source>Learning and soft computing: support vector machines, neural networks and fuzzy logic models</source>
          <year>2001</year>
          <publisher-loc>Massachusetts, and London, England</publisher-loc>
          <publisher-name>MIT Press Cambridge</publisher-name>
        </element-citation>
      </ref>
      <ref id="CR21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Specht</surname>
              <given-names>DF</given-names>
            </name>
          </person-group>
          <article-title>Probabilistic neural networks</article-title>
          <source>Neural Netw</source>
          <year>1990</year>
          <volume>1</volume>
          <issue>3</issue>
          <fpage>109</fpage>
          <lpage>18</lpage>
          <pub-id pub-id-type="doi">10.1016/0893-6080(90)90049-Q</pub-id>
        </element-citation>
      </ref>
      <ref id="CR22">
        <label>22.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wasserman</surname>
              <given-names>PD</given-names>
            </name>
          </person-group>
          <source>Advanced methods in neural networks</source>
          <year>1993</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Van Nostrand Reinhold</publisher-name>
        </element-citation>
      </ref>
      <ref id="CR23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Long</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Rosa</surname>
              <given-names>GJ</given-names>
            </name>
            <name>
              <surname>Weigel</surname>
              <given-names>KA</given-names>
            </name>
            <name>
              <surname>Avenda&#xF1;o</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Machine learning classification procedure for selecting SNPs in genomic selection: application to early mortality in broilers</article-title>
          <source>J of Animal Breeding and Genetics</source>
          <year>2007</year>
          <volume>124</volume>
          <fpage>377</fpage>
          <lpage>89</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1439-0388.2007.00694.x</pub-id>
        </element-citation>
      </ref>
      <ref id="CR24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gonz&#xE1;lez-Recio</surname>
              <given-names>O</given-names>
            </name>
            <name>
              <surname>Rosa</surname>
              <given-names>GJ</given-names>
            </name>
            <name>
              <surname>Gianola</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Machine learning methods and predictive ability metrics for genome-wide prediction of complex traits</article-title>
          <source>Livest Sci</source>
          <year>2014</year>
          <volume>166</volume>
          <fpage>217</fpage>
          <lpage>31</lpage>
          <pub-id pub-id-type="doi">10.1016/j.livsci.2014.05.036</pub-id>
        </element-citation>
      </ref>
      <ref id="CR25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ornella</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>P&#xE9;rez-Rodr&#xED;guez</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Tapia</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Gonz&#xE1;lez-Camacho</surname>
              <given-names>JM</given-names>
            </name>
            <name>
              <surname>Burgue&#xF1;o</surname>
              <given-names>J</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Genomic-enabled prediction with classification algorithms</article-title>
          <source>Heredity</source>
          <year>2014</year>
          <volume>112</volume>
          <issue>6</issue>
          <fpage>616</fpage>
          <lpage>26</lpage>
          <pub-id pub-id-type="doi">10.1038/hdy.2013.144</pub-id>
          <?supplied-pmid 24424163?>
          <pub-id pub-id-type="pmid">24424163</pub-id>
        </element-citation>
      </ref>
      <ref id="CR26">
        <label>26.</label>
        <mixed-citation publication-type="other">Brachi B, Geoffrey P, Morris GP, Borevitz JO. Genome-wide association studies in plants: the missing heritability is in the field. Genome Biology 2011; doi:10.1186/gb-2011-12-10-232.</mixed-citation>
      </ref>
      <ref id="CR27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moller</surname>
              <given-names>MF</given-names>
            </name>
          </person-group>
          <article-title>Scaled conjugate gradient algorithm for fast supervised learning</article-title>
          <source>Neural Netw</source>
          <year>1993</year>
          <volume>6</volume>
          <fpage>525</fpage>
          <lpage>33</lpage>
          <pub-id pub-id-type="doi">10.1016/S0893-6080(05)80056-5</pub-id>
        </element-citation>
      </ref>
      <ref id="CR28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Naftalyy</surname>
              <given-names>U</given-names>
            </name>
            <name>
              <surname>Intratorz</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Hornx</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Optimal ensemble averaging of neural networks</article-title>
          <source>Network Comput Neural Syst</source>
          <year>1997</year>
          <volume>8</volume>
          <fpage>283</fpage>
          <lpage>96</lpage>
          <pub-id pub-id-type="doi">10.1088/0954-898X_8_3_004</pub-id>
        </element-citation>
      </ref>
      <ref id="CR29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Parzen</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>On estimation of a probability density function and mode</article-title>
          <source>Ann Math Statist</source>
          <year>1962</year>
          <volume>33</volume>
          <fpage>1065</fpage>
          <lpage>76</lpage>
          <pub-id pub-id-type="doi">10.1214/aoms/1177704472</pub-id>
        </element-citation>
      </ref>
      <ref id="CR30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fawcett</surname>
              <given-names>T</given-names>
            </name>
          </person-group>
          <article-title>An introduction to ROC analysis</article-title>
          <source>Pattern Recogn Lett</source>
          <year>2006</year>
          <volume>27</volume>
          <issue>8</issue>
          <fpage>861</fpage>
          <lpage>74</lpage>
          <pub-id pub-id-type="doi">10.1016/j.patrec.2005.10.010</pub-id>
        </element-citation>
      </ref>
      <ref id="CR31">
        <label>31.</label>
        <mixed-citation publication-type="other">Murphy KP. Machine learning: a probabilistic perspective. 1st ed. Cambridge, Massachusetts, London, England: The MIT Press; 2012.</mixed-citation>
      </ref>
      <ref id="CR32">
        <label>32.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Davis</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Goadrich</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <source>The relationship between precision-recall and ROC curves. In: ICML &#x2018;06: Proceedings of the 23rd international conference on machine learning</source>
          <year>2006</year>
          <publisher-loc>New York, NY, USA</publisher-loc>
          <publisher-name>ACM</publisher-name>
        </element-citation>
      </ref>
      <ref id="CR33">
        <label>33.</label>
        <mixed-citation publication-type="other">Keilwagen J, Grosse I, Grau J. Area under precision-recall curves for weighted and unweighted Data. PLoS ONE 2014; doi:10.1371/journal.pone.0092209.</mixed-citation>
      </ref>
      <ref id="CR34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Heffner</surname>
              <given-names>EL</given-names>
            </name>
            <name>
              <surname>Lorenz</surname>
              <given-names>AJ</given-names>
            </name>
            <name>
              <surname>Jannink</surname>
              <given-names>J-L</given-names>
            </name>
            <name>
              <surname>Sorrells</surname>
              <given-names>ME</given-names>
            </name>
          </person-group>
          <article-title>Plant breeding with genomic selection: Gain per unit time and cost</article-title>
          <source>Crop Sci</source>
          <year>2010</year>
          <volume>50</volume>
          <fpage>1681</fpage>
          <lpage>90</lpage>
          <pub-id pub-id-type="doi">10.2135/cropsci2009.11.0662</pub-id>
        </element-citation>
      </ref>
      <ref id="CR35">
        <label>35.</label>
        <mixed-citation publication-type="other">Zhang X, P&#xE9;rez-Rodr&#xED;guez P, Semagn K, Beyene Y, Babu R, L&#xF3;pez-Cruz MA, San Vicente F, Olsen M, Buckler E, Jannink J-L, Prasanna BM and Crossa J. Genomic prediction in biparental tropical maize populations in water-stressed and well-watered environments using low-density and GBS SNPs. Heredity. 2015;114:291&#x2013;9.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
